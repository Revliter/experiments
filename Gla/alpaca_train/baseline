# Only train the classifiers
# Seems to have a bug here since we still add the initialized lora parts here?

Training Alpaca-LoRA model with params:
gla: True
====================
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
layerwise: True
train_layers: [[28], [29], [30], [31]]
lr_scaling: [1.0, 1.0, 1.0, 1.0]
====================
base_model: decapoda-research/llama-7b-hf
data_path: yahma/alpaca-cleaned
output_dir: ../ckpts/baseline
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca


Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:31<00:00,  1.05it/s]

Some weights of LlamaForCausalLM_GLA were not initialized from the model checkpoint at decapoda-research/llama-7b-hf and are newly initialized: ['gla_norms.1.weight', 'gla_norms.0.weight', 'gla_heads.1.weight', 'gla_heads.0.weight', 'gla_heads.2.weight', 'gla_norms.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 59.43it/s]
Loading cached split indices for dataset at /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow
trainable params: 524,304,384 || all params: 7,148,421,120 || trainable%: 7.334548079898236
Trainable parameters: ['base_model.model.model.norm.weight', 'base_model.model.lm_head.weight', 'base_model.model.gla_heads.0.weight', 'base_model.model.gla_heads.1.weight', 'base_model.model.gla_heads.2.weight', 'base_model.model.gla_norms.0.weight', 'base_model.model.gla_norms.1.weight', 'base_model.model.gla_norms.2.weight']

Map:   0%|          | 0/49760 [00:00<?, ? examples/s]
Map: 100%|█████████▉| 1991/2000 [00:02<00:00, 919.40 examples/s]
                                                                
wandb: Currently logged in as: 1249221036. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /scratch/users/nus/e0917621/zangwei/chenting/LLaMA-GLa/wandb/run-20231002_125654-p7mc0xxn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-flower-8
wandb: ⭐️ View project at https://wandb.ai/1249221036/huggingface
wandb: 🚀 View run at https://wandb.ai/1249221036/huggingface/runs/p7mc0xxn

  0%|          | 0/1164 [00:00<?, ?it/s]/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

  0%|          | 1/1164 [00:20<6:30:48, 20.16s/it]
 17%|█▋        | 200/1164 [57:51<6:33:50, 24.51s/it]
 Loss per layer: [11.069948196411133, 11.1168794631958, 11.019349098205566, 1.8155466318130493], Total loss: 35.021724700927734
Loss per layer: [10.306584358215332, 10.327335357666016, 10.260971069335938, 2.07527232170105], Total loss: 32.97016143798828
Loss per layer: [9.688848495483398, 9.650171279907227, 9.62238883972168, 1.5097312927246094], Total loss: 30.471139907836914
Loss per layer: [8.306026458740234, 8.36316967010498, 8.254899978637695, 1.4979937076568604], Total loss: 26.422090530395508
Loss per layer: [6.158829212188721, 6.169689655303955, 6.1536102294921875, 1.280604362487793], Total loss: 19.762733459472656
Loss per layer: [5.357886791229248, 5.393485069274902, 5.301976680755615, 1.1699116230010986], Total loss: 17.2232608795166
Loss per layer: [4.423916339874268, 4.457942962646484, 4.322576522827148, 1.0473703145980835], Total loss: 14.251805305480957
Loss per layer: [3.5028183460235596, 3.4675064086914062, 3.4966483116149902, 1.0300846099853516], Total loss: 11.49705696105957
Loss per layer: [3.0466129779815674, 2.9633262157440186, 2.9767916202545166, 1.0448724031448364], Total loss: 10.03160285949707
Loss per layer: [1.7506883144378662, 1.7935237884521484, 1.7452014684677124, 0.7218790054321289], Total loss: 6.011292457580566
Loss per layer: [2.1418354511260986, 2.1047956943511963, 2.067128896713257, 0.9651015996932983], Total loss: 7.2788615226745605
Loss per layer: [2.2117486000061035, 2.136596441268921, 2.143916368484497, 1.1240805387496948], Total loss: 7.616341590881348
Loss per layer: [1.5278825759887695, 1.587851643562317, 1.4951430559158325, 0.8513574004173279], Total loss: 5.4622344970703125
Loss per layer: [1.5938475131988525, 1.588334083557129, 1.5158045291900635, 0.9297851920127869], Total loss: 5.627771377563477
Loss per layer: [1.8180240392684937, 1.77027428150177, 1.7552051544189453, 1.121734857559204], Total loss: 6.465238571166992
Loss per layer: [1.5543122291564941, 1.5445530414581299, 1.4654145240783691, 0.8653696775436401], Total loss: 5.429649353027344
Loss per layer: [1.626706838607788, 1.6350210905075073, 1.526631474494934, 1.0503524541854858], Total loss: 5.838711738586426
Loss per layer: [1.4893112182617188, 1.477747917175293, 1.4322965145111084, 1.0324355363845825], Total loss: 5.431791305541992
Loss per layer: [1.7440913915634155, 1.7133095264434814, 1.6722989082336426, 0.9729169607162476], Total loss: 6.102616786956787
Loss per layer: [1.4468985795974731, 1.498220682144165, 1.4225308895111084, 1.0383576154708862], Total loss: 5.406007766723633


 34%|███▍      | 400/1164 [2:22:08<5:16:21, 24.84s/it]
 {'eval_loss': 0.9821382164955139, 'eval_runtime': 318.7215, 'eval_samples_per_second': 6.275, 'eval_steps_per_second': 0.784, 'epoch': 0.51}
Loss per layer: [1.761781096458435, 1.7046045064926147, 1.6749519109725952, 1.138224482536316], Total loss: 6.279561996459961
Loss per layer: [1.8909671306610107, 1.8251512050628662, 1.7782551050186157, 1.303725242614746], Total loss: 6.798098564147949
Loss per layer: [1.575310468673706, 1.5280717611312866, 1.5072641372680664, 1.144318699836731], Total loss: 5.754964828491211
Loss per layer: [1.9859836101531982, 1.971455693244934, 1.952918529510498, 1.220068097114563], Total loss: 7.130425930023193
Loss per layer: [1.7715215682983398, 1.6987848281860352, 1.6330636739730835, 1.0775080919265747], Total loss: 6.180878162384033
Loss per layer: [1.4507728815078735, 1.4185901880264282, 1.3776839971542358, 1.0514318943023682], Total loss: 5.298479080200195
Loss per layer: [1.4490373134613037, 1.4517226219177246, 1.4187088012695312, 1.0661896467208862], Total loss: 5.385658264160156
Loss per layer: [1.0748865604400635, 1.1070224046707153, 1.0581611394882202, 0.861304759979248], Total loss: 4.101375102996826
Loss per layer: [1.172594428062439, 1.1237436532974243, 1.0982816219329834, 0.8078166842460632], Total loss: 4.202436447143555
Loss per layer: [1.5722577571868896, 1.6007437705993652, 1.516195297241211, 1.1170178651809692], Total loss: 5.806214332580566
Loss per layer: [1.131208896636963, 1.1282912492752075, 1.0394988059997559, 0.7636234164237976], Total loss: 4.0626220703125
Loss per layer: [1.381530523300171, 1.348357081413269, 1.32767653465271, 1.068609356880188], Total loss: 5.12617301940918
Loss per layer: [1.0288138389587402, 1.0710889101028442, 1.0293463468551636, 0.7566553950309753], Total loss: 3.885904550552368
Loss per layer: [1.5100632905960083, 1.5237046480178833, 1.4409786462783813, 1.0913952589035034], Total loss: 5.5661420822143555
Loss per layer: [1.1154332160949707, 1.1427994966506958, 1.0847597122192383, 0.8409380912780762], Total loss: 4.183930397033691
Loss per layer: [1.2294703722000122, 1.2057862281799316, 1.1751861572265625, 0.9260375499725342], Total loss: 4.536479949951172
Loss per layer: [1.1820354461669922, 1.1965614557266235, 1.1659276485443115, 1.0144041776657104], Total loss: 4.558928489685059
Loss per layer: [1.4811809062957764, 1.4379926919937134, 1.4206210374832153, 0.9850096106529236], Total loss: 5.324804306030273
Loss per layer: [0.8749672770500183, 0.8914411664009094, 0.8586224317550659, 0.7332829833030701], Total loss: 3.358314037322998
Loss per layer: [1.191171407699585, 1.1443018913269043, 1.1464909315109253, 0.8789426684379578], Total loss: 4.360906600952148


 52%|█████▏    | 600/1164 [3:46:27<3:34:15, 22.79s/it]
 {'eval_loss': 0.9520882368087769, 'eval_runtime': 274.3131, 'eval_samples_per_second': 7.291, 'eval_steps_per_second': 0.911, 'epoch': 1.03}
Loss per layer: [1.1453673839569092, 1.168519377708435, 1.1342942714691162, 0.9062244296073914], Total loss: 4.354405403137207
Loss per layer: [1.4803757667541504, 1.459316372871399, 1.4501886367797852, 1.0748746395111084], Total loss: 5.464755058288574
Loss per layer: [1.0220510959625244, 1.0153992176055908, 1.0058239698410034, 0.8201364278793335], Total loss: 3.8634109497070312
Loss per layer: [1.0295425653457642, 1.0203876495361328, 1.0116333961486816, 0.8264948129653931], Total loss: 3.8880581855773926
Loss per layer: [1.2558314800262451, 1.2083507776260376, 1.1347565650939941, 0.6744940876960754], Total loss: 4.273433208465576
Loss per layer: [1.1000313758850098, 1.1395033597946167, 1.099636435508728, 0.863720715045929], Total loss: 4.202892303466797
Loss per layer: [1.0598130226135254, 1.0504299402236938, 1.0538673400878906, 0.7160133719444275], Total loss: 3.8801236152648926
Loss per layer: [1.186833143234253, 1.2083646059036255, 1.1439651250839233, 0.8780255317687988], Total loss: 4.41718864440918
Loss per layer: [0.8416681885719299, 0.8209472894668579, 0.8360450863838196, 0.751522958278656], Total loss: 3.250183582305908
Loss per layer: [1.4809422492980957, 1.4139114618301392, 1.3484965562820435, 0.9788590669631958], Total loss: 5.2222089767456055
{'loss': 1.0151, 'learning_rate': 0.00018721804511278195, 'epoch': 1.29}
Loss per layer: [1.225311279296875, 1.1928726434707642, 1.180405855178833, 0.9845859408378601], Total loss: 4.5831756591796875
Loss per layer: [1.1340606212615967, 1.0922572612762451, 1.0855827331542969, 0.8810332417488098], Total loss: 4.192934036254883
Loss per layer: [0.8687111139297485, 0.8595958352088928, 0.8530806303024292, 0.7623351216316223], Total loss: 3.3437225818634033
Loss per layer: [0.9014120101928711, 0.8968046307563782, 0.8735777139663696, 0.6842467188835144], Total loss: 3.356041193008423
Loss per layer: [0.9992551207542419, 0.995557963848114, 0.9300627708435059, 0.7470376491546631], Total loss: 3.6719133853912354
Loss per layer: [1.5311837196350098, 1.4968775510787964, 1.4457249641418457, 0.9554963111877441], Total loss: 5.4292826652526855
Loss per layer: [1.0902464389801025, 1.043007254600525, 1.0320351123809814, 0.7761879563331604], Total loss: 3.941476583480835
Loss per layer: [0.9659888744354248, 0.9692226648330688, 0.9375960230827332, 0.7270548343658447], Total loss: 3.5998623371124268
Loss per layer: [1.0153955221176147, 1.000946044921875, 0.9667613506317139, 0.7844001650810242], Total loss: 3.767503261566162
Loss per layer: [1.1899219751358032, 1.1525496244430542, 1.1427485942840576, 0.7574566006660461], Total loss: 4.242676734924316

 69%|██████▊   | 800/1164 [5:04:30<2:55:33, 28.94s/it]
 {'eval_loss': 0.9383078813552856, 'eval_runtime': 212.4902, 'eval_samples_per_second': 9.412, 'eval_steps_per_second': 1.177, 'epoch': 1.54}
Loss per layer: [0.695400595664978, 0.6752991676330566, 0.6789839863777161, 0.5650826692581177], Total loss: 2.6147665977478027
Loss per layer: [1.2269564867019653, 1.219547152519226, 1.2010177373886108, 0.9978384971618652], Total loss: 4.645359992980957
Loss per layer: [1.026747226715088, 1.0009384155273438, 0.9994255900382996, 0.7760528326034546], Total loss: 3.803164005279541
Loss per layer: [1.0293012857437134, 1.043959617614746, 1.0284050703048706, 0.9557122588157654], Total loss: 4.05737829208374
Loss per layer: [1.0737721920013428, 1.0611741542816162, 1.075187087059021, 0.9194037914276123], Total loss: 4.129537582397461
Loss per layer: [0.8036531209945679, 0.815924346446991, 0.8034228682518005, 0.6979979276657104], Total loss: 3.1209983825683594
Loss per layer: [1.0050346851348877, 0.9920616745948792, 0.9729547500610352, 0.8093287348747253], Total loss: 3.7793798446655273
Loss per layer: [0.8435100317001343, 0.8120866417884827, 0.8127953410148621, 0.7020983695983887], Total loss: 3.1704905033111572
Loss per layer: [1.0184834003448486, 0.9993054866790771, 0.9857662320137024, 0.8039084076881409], Total loss: 3.8074634075164795
Loss per layer: [1.2581599950790405, 1.2456263303756714, 1.220521330833435, 1.0191818475723267], Total loss: 4.7434892654418945
Loss per layer: [1.0733624696731567, 1.0501188039779663, 0.9918389916419983, 0.7891929745674133], Total loss: 3.904513120651245
Loss per layer: [1.2755297422409058, 1.285476565361023, 1.2377371788024902, 1.0770847797393799], Total loss: 4.875828266143799
Loss per layer: [1.1328552961349487, 1.1113007068634033, 1.1137908697128296, 0.8982662558555603], Total loss: 4.256213188171387
Loss per layer: [1.1565922498703003, 1.1239142417907715, 1.1032145023345947, 0.9138504862785339], Total loss: 4.297571659088135
Loss per layer: [0.8514825701713562, 0.835242748260498, 0.8332434296607971, 0.7607548236846924], Total loss: 3.2807235717773438
Loss per layer: [1.158737063407898, 1.1441009044647217, 1.0642709732055664, 0.7383126020431519], Total loss: 4.105421543121338
Loss per layer: [1.1751909255981445, 1.1505876779556274, 1.1508285999298096, 0.959924578666687], Total loss: 4.4365315437316895
Loss per layer: [0.8311690092086792, 0.8153067231178284, 0.8121612668037415, 0.6381257772445679], Total loss: 3.0967626571655273
Loss per layer: [1.2076494693756104, 1.1667542457580566, 1.120514154434204, 0.8053165078163147], Total loss: 4.300234317779541
Loss per layer: [0.6543933153152466, 0.6788209676742554, 0.6898316144943237, 0.5819671154022217], Total loss: 2.605013132095337


 86%|████████▌ | 1000/1164 [6:38:46<1:05:33, 23.99s/it]
 {'eval_loss': 0.9208502769470215, 'eval_runtime': 447.3476, 'eval_samples_per_second': 4.471, 'eval_steps_per_second': 0.559, 'epoch': 2.06}
Loss per layer: [1.04878830909729, 1.0517445802688599, 1.0112614631652832, 0.8257004618644714], Total loss: 3.937494993209839
Loss per layer: [1.0115489959716797, 0.9849258661270142, 0.9635239839553833, 0.7950764298439026], Total loss: 3.755075216293335
Loss per layer: [1.0846847295761108, 1.057760238647461, 1.01101815700531, 0.8479022979736328], Total loss: 4.001365661621094
Loss per layer: [0.6365993022918701, 0.6389865875244141, 0.6182404160499573, 0.502966046333313], Total loss: 2.396792411804199
Loss per layer: [0.83879554271698, 0.8013302683830261, 0.8188579678535461, 0.696666419506073], Total loss: 3.1556501388549805
Loss per layer: [0.7405858039855957, 0.7142292261123657, 0.7076141834259033, 0.5633895397186279], Total loss: 2.7258188724517822
Loss per layer: [1.0525152683258057, 1.0163371562957764, 1.0278316736221313, 0.7943803668022156], Total loss: 3.891064405441284
Loss per layer: [1.027948021888733, 1.0216631889343262, 1.0136065483093262, 0.7962944507598877], Total loss: 3.8595120906829834
Loss per layer: [0.952553391456604, 0.937249481678009, 0.9064770340919495, 0.6850811839103699], Total loss: 3.481361150741577
Loss per layer: [1.1470946073532104, 1.1316100358963013, 1.100111722946167, 0.9352364540100098], Total loss: 4.314052581787109
Loss per layer: [0.967742919921875, 0.9363526105880737, 0.9303986430168152, 0.7741138339042664], Total loss: 3.6086080074310303
Loss per layer: [0.911950409412384, 0.9006975293159485, 0.8860891461372375, 0.7003923058509827], Total loss: 3.3991293907165527
Loss per layer: [0.8668152093887329, 0.8423445224761963, 0.8282679319381714, 0.6560878157615662], Total loss: 3.1935155391693115
Loss per layer: [1.0347983837127686, 1.0192874670028687, 0.9920828342437744, 0.7760692238807678], Total loss: 3.822237730026245
Loss per layer: [0.999595582485199, 1.010487675666809, 1.0085172653198242, 0.7960877418518066], Total loss: 3.814688205718994
Loss per layer: [1.0368326902389526, 1.0246590375900269, 1.005791187286377, 0.6905564665794373], Total loss: 3.7578394412994385
Loss per layer: [1.0200536251068115, 1.0007365942001343, 0.9888213872909546, 0.820471465587616], Total loss: 3.830083131790161
Loss per layer: [1.1511824131011963, 1.1451590061187744, 1.1173720359802246, 0.9112241864204407], Total loss: 4.32493782043457
Loss per layer: [1.0770998001098633, 1.0712336301803589, 1.0442382097244263, 0.8786134719848633], Total loss: 4.071185111999512
Loss per layer: [0.7576500177383423, 0.757695198059082, 0.7381224036216736, 0.6139528751373291], Total loss: 2.8674204349517822
{'loss': 0.7857, 'learning_rate': 4.624060150375939e-05, 'epoch': 2.57}

100%|██████████| 1164/1164 [7:52:02<00:00, 24.33s/it]
{'eval_loss': 0.9121607542037964, 'eval_runtime': 354.2815, 'eval_samples_per_second': 5.645, 'eval_steps_per_second': 0.706, 'epoch': 2.57}
Loss per layer: [0.8571085929870605, 0.8498391509056091, 0.8270278573036194, 0.6955149173736572], Total loss: 3.2294905185699463
Loss per layer: [1.001434326171875, 0.9970835447311401, 0.9492589235305786, 0.8247443437576294], Total loss: 3.7725210189819336
Loss per layer: [0.6263840198516846, 0.5783351063728333, 0.5871606469154358, 0.4170662462711334], Total loss: 2.2089459896087646
Loss per layer: [0.9359588623046875, 0.9292784929275513, 0.9226915836334229, 0.6981266140937805], Total loss: 3.486055612564087
Loss per layer: [1.0110496282577515, 0.9821950197219849, 1.0125610828399658, 0.830048680305481], Total loss: 3.8358545303344727
Loss per layer: [0.7344186305999756, 0.7066806554794312, 0.6893450021743774, 0.6058576107025146], Total loss: 2.736301898956299
Loss per layer: [0.9347490072250366, 0.908183753490448, 0.9016573429107666, 0.7112312316894531], Total loss: 3.4558212757110596
Loss per layer: [0.799893319606781, 0.7916066646575928, 0.7987228035926819, 0.6477153897285461], Total loss: 3.037938117980957
Loss per layer: [0.9109491109848022, 0.8955796957015991, 0.9130154848098755, 0.7551285028457642], Total loss: 3.474672794342041
Loss per layer: [1.040536642074585, 1.0265185832977295, 1.0163263082504272, 0.8725315928459167], Total loss: 3.9559133052825928
Loss per layer: [1.0014082193374634, 0.9733811020851135, 0.9642472863197327, 0.7896010279655457], Total loss: 3.7286376953125
Loss per layer: [0.877769410610199, 0.846267819404602, 0.8613599538803101, 0.7066988348960876], Total loss: 3.2920961380004883
Loss per layer: [0.9207216501235962, 0.9089048504829407, 0.8833207488059998, 0.7065045237541199], Total loss: 3.419451951980591
Loss per layer: [1.1258853673934937, 1.1006592512130737, 1.088369369506836, 0.9111968874931335], Total loss: 4.226110935211182
Loss per layer: [0.8634393215179443, 0.8315637707710266, 0.8304010033607483, 0.6566547751426697], Total loss: 3.182058811187744
Loss per layer: [0.9490781426429749, 0.9281959533691406, 0.8930413722991943, 0.6779381036758423], Total loss: 3.448253631591797
{'train_runtime': 28302.8063, 'train_samples_per_second': 5.274, 'train_steps_per_second': 0.041, 'train_loss': 0.8775377699599642, 'epoch': 2.99}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▅▄▂▁
wandb:                   eval/runtime ▄▃▁█▅
wandb:        eval/samples_per_second ▄▅█▁▃
wandb:          eval/steps_per_second ▄▅█▁▃
wandb:                    train/epoch ▁▂▃▄▅▇▇█
wandb:              train/global_step ▁▂▃▄▅▇▇█
wandb:            train/learning_rate █▁
wandb:                     train/loss █▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.91216
wandb:                   eval/runtime 354.2815
wandb:        eval/samples_per_second 5.645
wandb:          eval/steps_per_second 0.706
wandb:                    train/epoch 2.99
wandb:              train/global_step 1164
wandb:            train/learning_rate 5e-05
wandb:                     train/loss 0.7857
wandb:               train/total_flos 1.5432774248772403e+18
wandb:               train/train_loss 0.87754
wandb:            train/train_runtime 28302.8063
wandb: train/train_samples_per_second 5.274
wandb:   train/train_steps_per_second 0.041
wandb: 
wandb: 🚀 View run sandy-flower-8 at: https://wandb.ai/1249221036/huggingface/runs/p7mc0xxn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231002_125654-p7mc0xxn/logs
