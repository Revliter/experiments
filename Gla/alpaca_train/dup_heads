# duplicate the last lm_head and norm to each block, then fix them and train lora

Training Alpaca-LoRA model with params:
gla: True
====================
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
layerwise: True
train_layers: [[28], [29], [30], [31]]
lr_scaling: [1.0, 1.0, 1.0, 1.0]
====================
base_model: decapoda-research/llama-7b-hf
data_path: yahma/alpaca-cleaned
output_dir: ../ckpts/dup_heads
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca


Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:38<00:00,  1.17s/it]
Some weights of LlamaForCausalLM_GLA were not initialized from the model checkpoint at decapoda-research/llama-7b-hf and are newly initialized: ['gla_heads.0.weight', 'gla_norms.2.weight', 'gla_heads.2.weight', 'gla_norms.0.weight', 'gla_heads.1.weight', 'gla_norms.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 80.58it/s]
Loading cached split indices for dataset at /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow
trainable params: 524,288 || all params: 7,132,168,192 || trainable%: 0.0073510324754831585
Trainable parameters: ['base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight']
                                                                
wandb: Currently logged in as: 1249221036. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /scratch/users/nus/e0917621/zangwei/chenting/LLaMA-GLa/wandb/run-20231002_234404-zqz67c1a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sponge-9
wandb: ⭐️ View project at https://wandb.ai/1249221036/huggingface
wandb: 🚀 View run at https://wandb.ai/1249221036/huggingface/runs/zqz67c1a

  0%|          | 0/1164 [00:00<?, ?it/s]/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

 17%|█▋        | 200/1164 [35:37<2:50:48, 10.63s/it]
 Loss per layer: [3.7320926189422607, 3.35662579536438, 2.9898719787597656, 2.027184247970581], Total loss: 12.105774879455566
Loss per layer: [3.5342564582824707, 3.0081214904785156, 2.5704424381256104, 1.7325681447982788], Total loss: 10.845388412475586
Loss per layer: [3.7675139904022217, 3.370084285736084, 2.9974634647369385, 2.0572831630706787], Total loss: 12.192344665527344
Loss per layer: [3.4756598472595215, 2.9305737018585205, 2.4399783611297607, 1.6109271049499512], Total loss: 10.45713996887207
Loss per layer: [3.975600481033325, 3.2463772296905518, 2.7299909591674805, 1.962835669517517], Total loss: 11.914804458618164
Loss per layer: [3.13535213470459, 2.3965396881103516, 2.1148312091827393, 1.3837594985961914], Total loss: 9.030482292175293
Loss per layer: [3.829540729522705, 2.583561658859253, 2.2525625228881836, 1.6238796710968018], Total loss: 10.289544105529785
Loss per layer: [2.909853458404541, 1.8078258037567139, 1.59983491897583, 1.095719337463379], Total loss: 7.413233280181885
Loss per layer: [3.4936530590057373, 1.9909383058547974, 1.7972561120986938, 1.355056881904602], Total loss: 8.6369047164917
Loss per layer: [4.217695236206055, 2.3328793048858643, 2.145144462585449, 1.5482490062713623], Total loss: 10.24396800994873
Loss per layer: [3.967822313308716, 2.0924124717712402, 1.900075078010559, 1.3674418926239014], Total loss: 9.327752113342285
Loss per layer: [3.6697194576263428, 1.8768815994262695, 1.6649202108383179, 1.2207187414169312], Total loss: 8.43224048614502
Loss per layer: [3.684056520462036, 1.6774319410324097, 1.5126489400863647, 1.0994369983673096], Total loss: 7.973574638366699
Loss per layer: [3.267460823059082, 1.7082526683807373, 1.5610109567642212, 1.1466907262802124], Total loss: 7.683415412902832
Loss per layer: [3.4053642749786377, 1.4838402271270752, 1.3878687620162964, 1.0464787483215332], Total loss: 7.323552131652832
Loss per layer: [3.6748688220977783, 1.9244484901428223, 1.7829506397247314, 1.396875262260437], Total loss: 8.779143333435059
Loss per layer: [3.4745030403137207, 1.579307198524475, 1.4661586284637451, 1.1591672897338867], Total loss: 7.679136276245117
Loss per layer: [2.696300506591797, 1.3168621063232422, 1.1777997016906738, 0.9350796341896057], Total loss: 6.126041889190674
Loss per layer: [3.641605854034424, 1.797548770904541, 1.6789653301239014, 1.3146861791610718], Total loss: 8.432806015014648
Loss per layer: [3.712526321411133, 1.6967830657958984, 1.5625735521316528, 1.287649154663086], Total loss: 8.25953197479248

 34%|███▍      | 400/1164 [1:15:36<2:43:46, 12.86s/it]
 {'eval_loss': 1.2244946956634521, 'eval_runtime': 96.0239, 'eval_samples_per_second': 20.828, 'eval_steps_per_second': 2.604, 'epoch': 0.51}
Loss per layer: [3.7723398208618164, 1.7025251388549805, 1.4990571737289429, 1.174322247505188], Total loss: 8.148244857788086
Loss per layer: [3.3145792484283447, 1.5509960651397705, 1.4299848079681396, 1.2099082469940186], Total loss: 7.505468368530273
Loss per layer: [3.597747802734375, 1.5716617107391357, 1.4354307651519775, 1.1658753156661987], Total loss: 7.770715713500977
Loss per layer: [4.597771167755127, 1.9098445177078247, 1.7500941753387451, 1.5769211053848267], Total loss: 9.834630966186523
Loss per layer: [3.344576120376587, 1.4523401260375977, 1.324265956878662, 1.1316258907318115], Total loss: 7.2528076171875
Loss per layer: [3.6202971935272217, 1.6264592409133911, 1.534810185432434, 1.3505045175552368], Total loss: 8.132071495056152
Loss per layer: [3.6726996898651123, 1.5545892715454102, 1.4128696918487549, 1.140797734260559], Total loss: 7.780956268310547
Loss per layer: [3.790238857269287, 1.4056121110916138, 1.2535810470581055, 1.1040433645248413], Total loss: 7.553475379943848
Loss per layer: [3.135655164718628, 1.3630422353744507, 1.2492395639419556, 1.076419711112976], Total loss: 6.824356555938721
Loss per layer: [3.107992172241211, 1.25070059299469, 1.1106094121932983, 0.9371833801269531], Total loss: 6.406485557556152
Loss per layer: [3.788097858428955, 1.3889460563659668, 1.2619529962539673, 1.0776829719543457], Total loss: 7.516679763793945
Loss per layer: [3.589118242263794, 1.3951772451400757, 1.256908893585205, 1.082678198814392], Total loss: 7.323882579803467
Loss per layer: [3.6536715030670166, 1.3593852519989014, 1.2272441387176514, 1.056319236755371], Total loss: 7.2966203689575195
Loss per layer: [3.694880723953247, 1.3458569049835205, 1.2011637687683105, 1.016144037246704], Total loss: 7.258045196533203
Loss per layer: [3.4537553787231445, 1.5183837413787842, 1.3780570030212402, 1.2000938653945923], Total loss: 7.550290107727051
Loss per layer: [3.0533125400543213, 1.3031697273254395, 1.2166215181350708, 1.0441697835922241], Total loss: 6.617273807525635
Loss per layer: [3.329511880874634, 1.43361234664917, 1.3168379068374634, 1.1407498121261597], Total loss: 7.220712184906006
Loss per layer: [3.147538900375366, 1.382806658744812, 1.2904547452926636, 1.0933644771575928], Total loss: 6.9141645431518555
Loss per layer: [3.339092969894409, 1.0903581380844116, 1.001075029373169, 0.8874117732048035], Total loss: 6.31793737411499
Loss per layer: [3.7031939029693604, 1.4775556325912476, 1.3853100538253784, 1.2291051149368286], Total loss: 7.795164585113525


 52%|█████▏    | 600/1164 [1:56:07<1:41:40, 10.82s/it]
 {'eval_loss': 1.1115561723709106, 'eval_runtime': 96.1766, 'eval_samples_per_second': 20.795, 'eval_steps_per_second': 2.599, 'epoch': 1.03}
Loss per layer: [3.726771354675293, 1.7291812896728516, 1.5726311206817627, 1.3464787006378174], Total loss: 8.375061988830566
Loss per layer: [3.985457420349121, 1.7286169528961182, 1.596831202507019, 1.3711910247802734], Total loss: 8.682096481323242
Loss per layer: [3.69229793548584, 1.4438790082931519, 1.3036597967147827, 1.1359940767288208], Total loss: 7.575830936431885
Loss per layer: [4.651982307434082, 1.021161437034607, 0.9008703827857971, 0.8723505735397339], Total loss: 7.446364879608154
Loss per layer: [4.311939716339111, 1.3786365985870361, 1.2428146600723267, 1.1018130779266357], Total loss: 8.03520393371582
Loss per layer: [3.3156638145446777, 1.3495351076126099, 1.2438404560089111, 1.0405504703521729], Total loss: 6.949589729309082
Loss per layer: [2.782308578491211, 1.1246403455734253, 1.0175517797470093, 0.8497132658958435], Total loss: 5.774214267730713
Loss per layer: [3.399256706237793, 1.3820534944534302, 1.2586321830749512, 1.0958752632141113], Total loss: 7.135817527770996
Loss per layer: [3.5254557132720947, 1.3853529691696167, 1.2667455673217773, 1.1422053575515747], Total loss: 7.319759368896484
Loss per layer: [3.9819893836975098, 1.3988500833511353, 1.2922879457473755, 1.1260610818862915], Total loss: 7.799188137054443
{'loss': 1.2643, 'learning_rate': 0.00018721804511278195, 'epoch': 1.29}
Loss per layer: [3.6739349365234375, 1.4268163442611694, 1.274096131324768, 1.070341944694519], Total loss: 7.445189476013184
Loss per layer: [3.192742347717285, 1.086691975593567, 1.0042535066604614, 0.8347570300102234], Total loss: 6.118444442749023
Loss per layer: [4.275003433227539, 1.4785785675048828, 1.3310115337371826, 1.2052323818206787], Total loss: 8.289826393127441
Loss per layer: [3.4746854305267334, 1.2660486698150635, 1.1306103467941284, 1.009540319442749], Total loss: 6.880885124206543
Loss per layer: [3.244926929473877, 1.4175937175750732, 1.301958680152893, 1.112662672996521], Total loss: 7.077141761779785
Loss per layer: [4.203006744384766, 1.5311673879623413, 1.3932411670684814, 1.239537000656128], Total loss: 8.366952896118164
Loss per layer: [3.1674673557281494, 1.3372622728347778, 1.2340341806411743, 1.0455870628356934], Total loss: 6.784350872039795
Loss per layer: [4.2165398597717285, 1.5435220003128052, 1.3984992504119873, 1.2512080669403076], Total loss: 8.409769058227539
Loss per layer: [3.6236302852630615, 1.1572514772415161, 1.069737434387207, 0.9673306941986084], Total loss: 6.817950248718262
Loss per layer: [3.552001953125, 1.4483582973480225, 1.3576719760894775, 1.1400028467178345], Total loss: 7.498034954071045


 69%|██████▊   | 799/1164 [2:33:20<1:04:49, 10.66s/it]
 69%|██████▊   | 800/1164 [2:33:31<1:04:38, 10.66s/it]
 {'eval_loss': 1.0975453853607178, 'eval_runtime': 96.1688, 'eval_samples_per_second': 20.797, 'eval_steps_per_second': 2.6, 'epoch': 1.54}
Loss per layer: [4.279150485992432, 1.4865468740463257, 1.3102905750274658, 1.1734968423843384], Total loss: 8.24948501586914
Loss per layer: [3.8295042514801025, 1.5078727006912231, 1.3600876331329346, 1.2040181159973145], Total loss: 7.901483058929443
Loss per layer: [3.9360313415527344, 1.47295343875885, 1.3352748155593872, 1.1882266998291016], Total loss: 7.932486057281494
Loss per layer: [3.8374104499816895, 1.4172868728637695, 1.279102087020874, 1.150297999382019], Total loss: 7.6840972900390625
Loss per layer: [3.415889263153076, 1.1701221466064453, 1.0284043550491333, 0.9319729208946228], Total loss: 6.546388626098633
Loss per layer: [3.138099431991577, 1.2756128311157227, 1.1422663927078247, 1.0034953355789185], Total loss: 6.559473991394043
Loss per layer: [3.723639488220215, 1.5203394889831543, 1.4094747304916382, 1.257664442062378], Total loss: 7.911118507385254
Loss per layer: [3.403640031814575, 1.3483597040176392, 1.2339284420013428, 1.0976852178573608], Total loss: 7.083613872528076
Loss per layer: [3.9657912254333496, 1.4141830205917358, 1.3043586015701294, 1.1588690280914307], Total loss: 7.843201637268066
Loss per layer: [3.384852409362793, 1.2050732374191284, 1.0810556411743164, 0.9528404474258423], Total loss: 6.62382173538208
Loss per layer: [3.5795180797576904, 1.4588242769241333, 1.3256477117538452, 1.1585161685943604], Total loss: 7.5225067138671875
Loss per layer: [3.287679672241211, 1.3155508041381836, 1.2050541639328003, 1.022783637046814], Total loss: 6.831068515777588
Loss per layer: [3.3461949825286865, 1.1881881952285767, 1.0667998790740967, 0.9451025128364563], Total loss: 6.546285629272461
Loss per layer: [3.6181046962738037, 1.3581249713897705, 1.2396759986877441, 1.0911800861358643], Total loss: 7.307085990905762
Loss per layer: [3.948960542678833, 1.274461269378662, 1.1677342653274536, 1.0490684509277344], Total loss: 7.440224647521973
Loss per layer: [3.9010987281799316, 1.3539355993270874, 1.227645754814148, 1.0941462516784668], Total loss: 7.576826572418213
Loss per layer: [3.4008491039276123, 1.3858987092971802, 1.239500641822815, 1.071632742881775], Total loss: 7.097881317138672
Loss per layer: [3.6602423191070557, 1.4666472673416138, 1.3546220064163208, 1.1880333423614502], Total loss: 7.6695451736450195
Loss per layer: [3.8614871501922607, 1.4945576190948486, 1.3885527849197388, 1.2132261991500854], Total loss: 7.957823753356934
Loss per layer: [3.5708227157592773, 1.3948582410812378, 1.2759745121002197, 1.1448900699615479], Total loss: 7.386545181274414


 86%|████████▌ | 1000/1164 [3:10:43<29:02, 10.63s/it]
 {'eval_loss': 1.0933605432510376, 'eval_runtime': 96.1598, 'eval_samples_per_second': 20.799, 'eval_steps_per_second': 2.6, 'epoch': 2.06}
Loss per layer: [3.261094808578491, 1.4074355363845825, 1.2790436744689941, 1.1247167587280273], Total loss: 7.072290897369385
Loss per layer: [3.7247753143310547, 1.433789849281311, 1.3033208847045898, 1.139725685119629], Total loss: 7.601611614227295
Loss per layer: [3.646225690841675, 1.389917016029358, 1.2989654541015625, 1.1390137672424316], Total loss: 7.474122047424316
Loss per layer: [3.6213254928588867, 1.306456208229065, 1.1631052494049072, 1.0062141418457031], Total loss: 7.097101211547852
Loss per layer: [3.4188785552978516, 1.4202364683151245, 1.2802667617797852, 1.1250650882720947], Total loss: 7.244446754455566
Loss per layer: [4.029869556427002, 1.3002707958221436, 1.1743918657302856, 1.0625667572021484], Total loss: 7.567098617553711
Loss per layer: [3.2977428436279297, 1.391676902770996, 1.2551512718200684, 1.0696460008621216], Total loss: 7.014216899871826
Loss per layer: [3.832594394683838, 1.4549084901809692, 1.328184962272644, 1.1250512599945068], Total loss: 7.740738868713379
Loss per layer: [3.5162973403930664, 1.3256900310516357, 1.1755456924438477, 1.0465006828308105], Total loss: 7.0640339851379395
Loss per layer: [3.1994762420654297, 1.2322264909744263, 1.1105670928955078, 0.9696442484855652], Total loss: 6.511913776397705
Loss per layer: [3.193053722381592, 1.2011654376983643, 1.0927225351333618, 0.9659626483917236], Total loss: 6.45290470123291
Loss per layer: [3.4849836826324463, 1.2072703838348389, 1.0913923978805542, 0.9914889335632324], Total loss: 6.775135517120361
Loss per layer: [3.7552647590637207, 1.615870475769043, 1.4838066101074219, 1.265721082687378], Total loss: 8.120662689208984
Loss per layer: [3.5037496089935303, 1.2214000225067139, 1.113410234451294, 1.0193711519241333], Total loss: 6.857931137084961
Loss per layer: [3.923215389251709, 1.5190045833587646, 1.3970134258270264, 1.2290949821472168], Total loss: 8.068328857421875
Loss per layer: [3.7321412563323975, 1.5046101808547974, 1.3554449081420898, 1.1861099004745483], Total loss: 7.778306484222412
Loss per layer: [3.7315316200256348, 1.4295798540115356, 1.287231206893921, 1.1463654041290283], Total loss: 7.594707489013672
Loss per layer: [3.6299052238464355, 1.3279374837875366, 1.2091224193572998, 1.0861790180206299], Total loss: 7.253144264221191
Loss per layer: [3.8167145252227783, 1.409463882446289, 1.2817394733428955, 1.1452656984329224], Total loss: 7.6531829833984375
Loss per layer: [3.695258378982544, 1.505072832107544, 1.3026257753372192, 1.1479876041412354], Total loss: 7.650944709777832
{'loss': 1.0845, 'learning_rate': 4.624060150375939e-05, 'epoch': 2.57}


100%|██████████| 1164/1164 [3:44:49<00:00, 11.59s/it]
{'eval_loss': 1.0915871858596802, 'eval_runtime': 96.1884, 'eval_samples_per_second': 20.793, 'eval_steps_per_second': 2.599, 'epoch': 2.57}
Loss per layer: [4.170299053192139, 1.5382691621780396, 1.3961502313613892, 1.2755333185195923], Total loss: 8.38025188446045
Loss per layer: [3.484900951385498, 1.1388405561447144, 1.036621332168579, 0.9122210144996643], Total loss: 6.57258415222168
Loss per layer: [3.1521787643432617, 1.2410792112350464, 1.1350562572479248, 0.9662147164344788], Total loss: 6.4945292472839355
Loss per layer: [3.6766178607940674, 1.456318736076355, 1.3210211992263794, 1.1025276184082031], Total loss: 7.556485176086426
Loss per layer: [3.545851469039917, 1.3889652490615845, 1.2582478523254395, 1.1109336614608765], Total loss: 7.3039984703063965
Loss per layer: [3.163389205932617, 1.2756774425506592, 1.1714528799057007, 1.0479880571365356], Total loss: 6.658507823944092
Loss per layer: [3.526383638381958, 1.2702206373214722, 1.121241569519043, 0.9937479496002197], Total loss: 6.911593437194824
Loss per layer: [3.8755273818969727, 1.286765456199646, 1.171383023262024, 1.0403944253921509], Total loss: 7.374070167541504
Loss per layer: [3.5264031887054443, 1.3870513439178467, 1.2521004676818848, 1.1219810247421265], Total loss: 7.287536144256592
Loss per layer: [3.2872862815856934, 1.2338950634002686, 1.1140602827072144, 0.9676270484924316], Total loss: 6.602868556976318
Loss per layer: [4.587609767913818, 0.9016616940498352, 0.798645555973053, 0.7927141785621643], Total loss: 7.080631256103516
Loss per layer: [3.9769866466522217, 1.5829846858978271, 1.447722315788269, 1.2452116012573242], Total loss: 8.252904891967773
Loss per layer: [3.784327507019043, 1.4618597030639648, 1.3384432792663574, 1.1272985935211182], Total loss: 7.7119293212890625
Loss per layer: [3.9228951930999756, 1.463331699371338, 1.3410849571228027, 1.1849381923675537], Total loss: 7.912249565124512
Loss per layer: [3.7522711753845215, 1.4078021049499512, 1.261955738067627, 1.134544849395752], Total loss: 7.556573867797852
Loss per layer: [3.4667952060699463, 1.3301459550857544, 1.2159500122070312, 1.0421582460403442], Total loss: 7.055049419403076
{'train_runtime': 13493.2238, 'train_samples_per_second': 11.063, 'train_steps_per_second': 0.086, 'train_loss': 1.1611606689662868, 'epoch': 2.99}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁
wandb:                   eval/runtime ▁▇▇▇█
wandb:        eval/samples_per_second █▁▂▂▁
wandb:          eval/steps_per_second █▁▂▂▁
wandb:                    train/epoch ▁▂▃▄▅▇▇█
wandb:              train/global_step ▁▂▃▄▅▇▇█
wandb:            train/learning_rate █▁
wandb:                     train/loss █▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 1.09159
wandb:                   eval/runtime 96.1884
wandb:        eval/samples_per_second 20.793
wandb:          eval/steps_per_second 2.599
wandb:                    train/epoch 2.99
wandb:              train/global_step 1164
wandb:            train/learning_rate 5e-05
wandb:                     train/loss 1.0845
wandb:               train/total_flos 1.540920884667089e+18
wandb:               train/train_loss 1.16116
wandb:            train/train_runtime 13493.2238
wandb: train/train_samples_per_second 11.063
wandb:   train/train_steps_per_second 0.086
wandb: 
wandb: 🚀 View run apricot-sponge-9 at: https://wandb.ai/1249221036/huggingface/runs/zqz67c1a
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231002_234404-zqz67c1a/logs
