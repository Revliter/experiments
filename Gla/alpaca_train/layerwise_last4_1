# Layerwise Last4 lr_scaling [0.4, 0.6, 0.8, 1.0]

Training Alpaca-LoRA model with params:
gla: True
====================
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
layerwise: True
train_layers: [[28], [29], [30], [31]]
lr_scaling: [0.4, 0.6, 0.8, 1.0]
====================
base_model: decapoda-research/llama-7b-hf
data_path: yahma/alpaca-cleaned
output_dir: ../ckpts/last4-lr_scale-alpaca
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca


Loading checkpoint shards: 100%|██████████| 33/33 [00:36<00:00,  1.11s/it]
Some weights of LlamaForCausalLM_GLA were not initialized from the model checkpoint at decapoda-research/llama-7b-hf and are newly initialized: ['gla_norms.0.weight', 'gla_heads.0.weight', 'gla_norms.1.weight', 'gla_heads.2.weight', 'gla_norms.2.weight', 'gla_heads.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 22.24it/s]
Loading cached split indices for dataset at /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow
trainable params: 524,828,672 || all params: 7,132,168,192 || trainable%: 7.3586132277235
Trainable parameters: ['base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight', 'base_model.model.gla_heads.0.weight', 'base_model.model.gla_heads.1.weight', 'base_model.model.gla_heads.2.weight', 'base_model.model.gla_norms.0.weight', 'base_model.model.gla_norms.1.weight', 'base_model.model.gla_norms.2.weight']
                                                                
wandb: Currently logged in as: 1249221036. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /scratch/users/nus/e0917621/zangwei/chenting/LLaMA-GLa/wandb/run-20231001_094800-bvqzymj7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sun-5
wandb: ⭐️ View project at https://wandb.ai/1249221036/huggingface
wandb: 🚀 View run at https://wandb.ai/1249221036/huggingface/runs/bvqzymj7


 17%|█▋        | 200/1164 [37:00<2:57:14, 11.03s/it]
 Loss per layer: [11.05180835723877, 11.116767883300781, 10.966496467590332, 1.7415070533752441], Total loss: 34.87657928466797
Loss per layer: [10.811559677124023, 10.720043182373047, 10.548248291015625, 1.8587303161621094], Total loss: 33.93857955932617
Loss per layer: [10.494754791259766, 10.149114608764648, 9.7857084274292, 1.3519237041473389], Total loss: 31.7814998626709
Loss per layer: [10.012360572814941, 9.397246360778809, 8.82481861114502, 1.151322364807129], Total loss: 29.385746002197266
Loss per layer: [8.750587463378906, 7.665768146514893, 6.522989273071289, 1.0424413681030273], Total loss: 23.98178482055664
Loss per layer: [8.021238327026367, 6.531611919403076, 5.348755836486816, 0.9930059313774109], Total loss: 20.89461326599121
Loss per layer: [7.002370834350586, 5.3369059562683105, 4.371429920196533, 1.2163300514221191], Total loss: 17.92703628540039
Loss per layer: [5.002752304077148, 3.515627145767212, 2.792844533920288, 1.0684548616409302], Total loss: 12.379678726196289
Loss per layer: [4.828767776489258, 3.4683640003204346, 2.9093265533447266, 1.0845046043395996], Total loss: 12.290962219238281
Loss per layer: [3.9078986644744873, 2.800354480743408, 2.2708663940429688, 0.9404510855674744], Total loss: 9.919569969177246
Loss per layer: [4.288029670715332, 3.44195556640625, 3.074113130569458, 1.139481544494629], Total loss: 11.94357967376709
Loss per layer: [3.7646214962005615, 2.975325107574463, 2.587003231048584, 1.1455786228179932], Total loss: 10.472528457641602
Loss per layer: [2.9897401332855225, 2.4503653049468994, 2.155329704284668, 1.3455054759979248], Total loss: 8.940940856933594
Loss per layer: [2.9064042568206787, 2.342578411102295, 2.0813002586364746, 1.2922943830490112], Total loss: 8.622576713562012
Loss per layer: [2.736067056655884, 2.1047353744506836, 1.884336233139038, 0.9821013808250427], Total loss: 7.707240104675293
Loss per layer: [2.8204658031463623, 2.2454771995544434, 1.9869110584259033, 1.001217007637024], Total loss: 8.054070472717285
Loss per layer: [3.0972042083740234, 2.4942142963409424, 2.275437116622925, 0.9997855424880981], Total loss: 8.8666410446167
Loss per layer: [1.938619613647461, 1.5422742366790771, 1.3382635116577148, 0.9112370610237122], Total loss: 5.7303948402404785
Loss per layer: [2.906181812286377, 2.4794728755950928, 2.1994338035583496, 1.1954014301300049], Total loss: 8.780489921569824
Loss per layer: [1.9197218418121338, 1.5808950662612915, 1.4182453155517578, 0.9094177484512329], Total loss: 5.828279972076416


 34%|███▍      | 400/1164 [1:18:57<2:44:27, 12.92s/it]
 {'eval_loss': 1.0050121545791626, 'eval_runtime': 95.7018, 'eval_samples_per_second': 20.898, 'eval_steps_per_second': 2.612, 'epoch': 0.51}
Loss per layer: [2.3224024772644043, 1.9331796169281006, 1.733022689819336, 1.096383810043335], Total loss: 7.084988594055176
Loss per layer: [1.7451258897781372, 1.4812486171722412, 1.3818422555923462, 0.9097402691841125], Total loss: 5.5179572105407715
Loss per layer: [1.9102520942687988, 1.565063714981079, 1.4103752374649048, 0.9730175137519836], Total loss: 5.85870885848999
Loss per layer: [2.438199996948242, 1.9100368022918701, 1.7053111791610718, 0.8774290084838867], Total loss: 6.9309773445129395
Loss per layer: [2.1813302040100098, 1.849205493927002, 1.6883102655410767, 1.1525912284851074], Total loss: 6.871437072753906
Loss per layer: [2.211217164993286, 1.815574288368225, 1.5899657011032104, 0.9161641597747803], Total loss: 6.53292179107666
Loss per layer: [2.3897268772125244, 2.095065116882324, 1.927713394165039, 1.3364887237548828], Total loss: 7.748993873596191
Loss per layer: [1.905105710029602, 1.6180330514907837, 1.4356518983840942, 0.901549756526947], Total loss: 5.860340595245361
Loss per layer: [2.2274789810180664, 1.8532354831695557, 1.6888591051101685, 1.0371248722076416], Total loss: 6.806697845458984
Loss per layer: [1.6300851106643677, 1.3671503067016602, 1.2936288118362427, 0.9470642805099487], Total loss: 5.23792839050293
Loss per layer: [1.2644052505493164, 1.0639965534210205, 1.0593657493591309, 0.7285652160644531], Total loss: 4.1163330078125
Loss per layer: [2.499724864959717, 2.1083877086639404, 1.9092079401016235, 1.0004914999008179], Total loss: 7.5178117752075195
Loss per layer: [1.7427144050598145, 1.483712077140808, 1.4304639101028442, 1.014436960220337], Total loss: 5.671327590942383
Loss per layer: [1.8728362321853638, 1.4275835752487183, 1.1816209554672241, 0.7711278796195984], Total loss: 5.25316858291626
Loss per layer: [1.8758565187454224, 1.5796023607254028, 1.4647784233093262, 0.9798834323883057], Total loss: 5.900120735168457
Loss per layer: [1.972549557685852, 1.621888279914856, 1.4649063348770142, 0.99821537733078], Total loss: 6.057559490203857
Loss per layer: [1.3402493000030518, 1.1318557262420654, 1.1177719831466675, 0.8486208319664001], Total loss: 4.438498020172119
Loss per layer: [1.4927284717559814, 1.2874149084091187, 1.2000702619552612, 1.0779643058776855], Total loss: 5.058177947998047
Loss per layer: [1.743249773979187, 1.4944175481796265, 1.359121322631836, 0.9278945326805115], Total loss: 5.524682998657227
Loss per layer: [1.4655530452728271, 1.2209903001785278, 1.097857117652893, 0.7984409332275391], Total loss: 4.582841396331787


 52%|█████▏    | 600/1164 [2:01:32<1:47:03, 11.39s/it]
 {'eval_loss': 0.9783755540847778, 'eval_runtime': 95.6593, 'eval_samples_per_second': 20.908, 'eval_steps_per_second': 2.613, 'epoch': 1.03}
Loss per layer: [1.4723727703094482, 1.2602055072784424, 1.1843287944793701, 0.9887221455574036], Total loss: 4.9056291580200195
Loss per layer: [1.4606940746307373, 1.2583787441253662, 1.096325159072876, 0.9059075713157654], Total loss: 4.7213053703308105
Loss per layer: [1.7297099828720093, 1.4700183868408203, 1.3720418214797974, 1.1076362133026123], Total loss: 5.67940616607666
Loss per layer: [1.4900264739990234, 1.204253077507019, 1.0690326690673828, 0.8517056703567505], Total loss: 4.615017890930176
Loss per layer: [1.2639964818954468, 1.0175617933273315, 0.9394376277923584, 0.7845565676689148], Total loss: 4.005552291870117
Loss per layer: [1.855980634689331, 1.6490614414215088, 1.502301573753357, 1.193516731262207], Total loss: 6.200860500335693
Loss per layer: [1.5031027793884277, 1.3446284532546997, 1.2142674922943115, 1.0342824459075928], Total loss: 5.096281051635742
Loss per layer: [1.4128625392913818, 1.2503831386566162, 1.1300427913665771, 0.8439124226570129], Total loss: 4.637200832366943
Loss per layer: [1.584599494934082, 1.2988442182540894, 1.170486330986023, 0.8635528087615967], Total loss: 4.917483329772949
Loss per layer: [1.574357271194458, 1.3435579538345337, 1.1963787078857422, 0.9260842800140381], Total loss: 5.040378570556641
{'loss': 1.018, 'learning_rate': 7.488721804511278e-05, 'epoch': 1.29}
Loss per layer: [1.6384953260421753, 1.3991711139678955, 1.3580197095870972, 1.0746333599090576], Total loss: 5.470319747924805
Loss per layer: [1.6919593811035156, 1.3726550340652466, 1.1968475580215454, 0.762536883354187], Total loss: 5.023998737335205
Loss per layer: [1.5305285453796387, 1.2907946109771729, 1.20906400680542, 0.9480509757995605], Total loss: 4.978437900543213
Loss per layer: [1.2242087125778198, 0.9125875234603882, 0.8222841620445251, 0.6227449178695679], Total loss: 3.5818252563476562
Loss per layer: [0.9334375262260437, 0.7332101464271545, 0.6331863403320312, 0.48112136125564575], Total loss: 2.7809553146362305
Loss per layer: [1.4457935094833374, 1.26384437084198, 1.1719911098480225, 0.943130373954773], Total loss: 4.824759483337402
Loss per layer: [1.1896283626556396, 0.9748899936676025, 0.8872191905975342, 0.6641124486923218], Total loss: 3.7158498764038086
Loss per layer: [1.4877378940582275, 1.3230273723602295, 1.2303369045257568, 0.9898058176040649], Total loss: 5.030908107757568
Loss per layer: [1.4360098838806152, 1.2535220384597778, 1.1580665111541748, 0.9739800691604614], Total loss: 4.821578502655029
Loss per layer: [1.584682583808899, 1.33729088306427, 1.240465521812439, 0.9712727665901184], Total loss: 5.133711814880371


 69%|██████▊   | 800/1164 [2:42:14<1:09:53, 11.52s/it]
 {'eval_loss': 0.9705930352210999, 'eval_runtime': 95.7094, 'eval_samples_per_second': 20.897, 'eval_steps_per_second': 2.612, 'epoch': 1.54}
Loss per layer: [1.4455459117889404, 1.2308733463287354, 1.1621894836425781, 0.9786010980606079], Total loss: 4.817209720611572
Loss per layer: [1.1238536834716797, 0.9359318017959595, 0.8231528997421265, 0.6913209557533264], Total loss: 3.5742592811584473
Loss per layer: [1.6191753149032593, 1.346536636352539, 1.2183747291564941, 0.9285076260566711], Total loss: 5.1125946044921875
Loss per layer: [1.3074716329574585, 1.0631225109100342, 0.9965063333511353, 0.7507138848304749], Total loss: 4.117814064025879
Loss per layer: [1.478357195854187, 1.179171085357666, 0.9555121660232544, 0.6128618717193604], Total loss: 4.225902557373047
Loss per layer: [1.3241982460021973, 1.1132208108901978, 1.0130277872085571, 0.7970718145370483], Total loss: 4.247518539428711
Loss per layer: [1.5753943920135498, 1.3574073314666748, 1.238013744354248, 0.8939699530601501], Total loss: 5.064785480499268
Loss per layer: [1.3027409315109253, 1.1077255010604858, 1.0563757419586182, 0.8913148045539856], Total loss: 4.358157157897949
Loss per layer: [1.3375030755996704, 1.1556308269500732, 1.1028034687042236, 0.9104399085044861], Total loss: 4.506377220153809
Loss per layer: [1.3194829225540161, 1.1000744104385376, 0.9741576910018921, 0.7514635324478149], Total loss: 4.145178318023682
Loss per layer: [1.503904938697815, 1.2878557443618774, 1.2207720279693604, 0.9711509943008423], Total loss: 4.9836835861206055
Loss per layer: [1.4696366786956787, 1.2236570119857788, 1.0686120986938477, 0.8834064602851868], Total loss: 4.645312309265137
Loss per layer: [1.8433818817138672, 1.4912426471710205, 1.3433446884155273, 0.9932004809379578], Total loss: 5.671169281005859
Loss per layer: [1.2076622247695923, 1.0332821607589722, 0.9559856653213501, 0.7717723250389099], Total loss: 3.9687023162841797
Loss per layer: [1.6124738454818726, 1.2794668674468994, 1.1092236042022705, 0.785621166229248], Total loss: 4.78678560256958
Loss per layer: [1.4651144742965698, 1.3014518022537231, 1.236523151397705, 1.0500032901763916], Total loss: 5.053092956542969
Loss per layer: [1.4033920764923096, 1.12618088722229, 1.0288113355636597, 0.7169924974441528], Total loss: 4.275376796722412
Loss per layer: [1.1644788980484009, 1.0209084749221802, 0.9234646558761597, 0.7721934914588928], Total loss: 3.881045341491699
Loss per layer: [1.2398943901062012, 1.0351616144180298, 0.9659725427627563, 0.7390116453170776], Total loss: 3.9800400733947754
Loss per layer: [1.2937480211257935, 1.055515170097351, 0.966799259185791, 0.717086672782898], Total loss: 4.033149242401123
                    

 86%|████████▌ | 1000/1164 [3:22:17<33:27, 12.24s/it]{'eval_loss': 0.9618425965309143, 'eval_runtime': 95.9163, 'eval_samples_per_second': 20.852, 'eval_steps_per_second': 2.606, 'epoch': 2.06}
Loss per layer: [1.2201610803604126, 1.099755048751831, 1.046647071838379, 0.8855516910552979], Total loss: 4.252115249633789
Loss per layer: [1.1166391372680664, 0.8155977725982666, 0.6832886338233948, 0.49373266100883484], Total loss: 3.1092581748962402
Loss per layer: [1.2272428274154663, 1.0211435556411743, 0.9277138113975525, 0.7720672488212585], Total loss: 3.948167562484741
Loss per layer: [1.0830111503601074, 0.8748491406440735, 0.8103528618812561, 0.6658321022987366], Total loss: 3.4340450763702393
Loss per layer: [1.4919451475143433, 1.2613403797149658, 1.163701057434082, 0.8655709028244019], Total loss: 4.782557487487793
Loss per layer: [1.3155184984207153, 1.1776585578918457, 1.0944169759750366, 0.8951742053031921], Total loss: 4.4827680587768555
Loss per layer: [0.8452660441398621, 0.6601405739784241, 0.6027002930641174, 0.5037283897399902], Total loss: 2.611835241317749
Loss per layer: [1.4209777116775513, 1.239489197731018, 1.1474347114562988, 0.968220055103302], Total loss: 4.776121616363525
Loss per layer: [1.180973768234253, 1.0366957187652588, 0.9508063793182373, 0.7793886661529541], Total loss: 3.947864532470703
Loss per layer: [1.2997233867645264, 1.0137476921081543, 0.9131752848625183, 0.7105184197425842], Total loss: 3.937164783477783
Loss per layer: [1.4590452909469604, 1.1319477558135986, 1.016660451889038, 0.7440137267112732], Total loss: 4.3516669273376465
Loss per layer: [1.158595085144043, 0.9381428360939026, 0.8541713953018188, 0.6264947056770325], Total loss: 3.5774037837982178
Loss per layer: [1.5501906871795654, 1.2903434038162231, 1.1172821521759033, 0.8243741989135742], Total loss: 4.782190322875977
Loss per layer: [1.578352689743042, 1.3718812465667725, 1.256363868713379, 0.9945611357688904], Total loss: 5.2011590003967285
Loss per layer: [1.3142085075378418, 1.1085139513015747, 0.9942866563796997, 0.7592561841011047], Total loss: 4.176265239715576
Loss per layer: [1.099661946296692, 0.8477769494056702, 0.7948836088180542, 0.5415235161781311], Total loss: 3.283845901489258
Loss per layer: [1.2390719652175903, 1.0262702703475952, 0.9608427882194519, 0.7671964168548584], Total loss: 3.9933815002441406
Loss per layer: [1.0980722904205322, 0.8883141279220581, 0.8142867088317871, 0.7244508862495422], Total loss: 3.5251238346099854
Loss per layer: [1.3577666282653809, 1.0925356149673462, 0.967512845993042, 0.7159090638160706], Total loss: 4.133724212646484
Loss per layer: [0.9889374375343323, 0.8370752334594727, 0.7572393417358398, 0.6374073624610901], Total loss: 3.2206592559814453
{'loss': 0.8272, 'learning_rate': 1.8496240601503758e-05, 'epoch': 2.57}


100%|██████████| 1164/1164 [3:56:25<00:00, 12.19s/it]
{'eval_loss': 0.9581999778747559, 'eval_runtime': 95.7382, 'eval_samples_per_second': 20.89, 'eval_steps_per_second': 2.611, 'epoch': 2.57}
Loss per layer: [1.0474586486816406, 0.876479983329773, 0.7761174440383911, 0.6501699090003967], Total loss: 3.3502259254455566
Loss per layer: [1.1998157501220703, 1.0589277744293213, 0.9835488796234131, 0.823718786239624], Total loss: 4.066011428833008
Loss per layer: [1.1665922403335571, 0.9625217914581299, 0.90702223777771, 0.7453169822692871], Total loss: 3.7814533710479736
Loss per layer: [1.1108672618865967, 0.9173086881637573, 0.8539014458656311, 0.7486273646354675], Total loss: 3.630704641342163
Loss per layer: [1.4494454860687256, 1.1830180883407593, 1.0646902322769165, 0.7769151926040649], Total loss: 4.474068641662598
Loss per layer: [1.141564965248108, 1.0104434490203857, 0.9774271845817566, 0.8005579710006714], Total loss: 3.9299936294555664
Loss per layer: [1.2275214195251465, 1.0351765155792236, 0.9443123936653137, 0.7321915626525879], Total loss: 3.939201831817627
Loss per layer: [1.2892916202545166, 1.0979551076889038, 1.0200166702270508, 0.8343925476074219], Total loss: 4.2416558265686035
Loss per layer: [0.7576054930686951, 0.5880391001701355, 0.5354055762290955, 0.4194434583187103], Total loss: 2.3004934787750244
Loss per layer: [1.0952842235565186, 0.9097577929496765, 0.8258621096611023, 0.6858438849449158], Total loss: 3.5167481899261475
Loss per layer: [1.1465086936950684, 0.9264085292816162, 0.8137125968933105, 0.6609836220741272], Total loss: 3.5476133823394775
Loss per layer: [1.3632348775863647, 1.16340970993042, 1.0959457159042358, 0.8825799822807312], Total loss: 4.5051703453063965
Loss per layer: [1.1060786247253418, 0.897173285484314, 0.8303931951522827, 0.6281925439834595], Total loss: 3.4618377685546875
Loss per layer: [1.2081305980682373, 1.0041886568069458, 0.9278894662857056, 0.7567740678787231], Total loss: 3.8969826698303223
Loss per layer: [1.2241986989974976, 1.0533297061920166, 0.9716271162033081, 0.7979222536087036], Total loss: 4.047077655792236
Loss per layer: [1.2830743789672852, 1.091564416885376, 1.0320801734924316, 0.854760468006134], Total loss: 4.261479377746582
{'train_runtime': 14189.9701, 'train_samples_per_second': 10.52, 'train_steps_per_second': 0.082, 'train_loss': 0.9024357156655223, 'epoch': 2.99}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▄▃▂▁
wandb:                   eval/runtime ▂▁▂█▃
wandb:        eval/samples_per_second ▇█▇▁▆
wandb:          eval/steps_per_second ▇█▇▁▆
wandb:                    train/epoch ▁▂▃▄▅▇▇█
wandb:              train/global_step ▁▂▃▄▅▇▇█
wandb:            train/learning_rate █▁
wandb:                     train/loss █▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.9582
wandb:                   eval/runtime 95.7382
wandb:        eval/samples_per_second 20.89
wandb:          eval/steps_per_second 2.611
wandb:                    train/epoch 2.99
wandb:              train/global_step 1164
wandb:            train/learning_rate 2e-05
wandb:                     train/loss 0.8272
wandb:               train/total_flos 1.5405498825776824e+18
wandb:               train/train_loss 0.90244
wandb:            train/train_runtime 14189.9701
wandb: train/train_samples_per_second 10.52
wandb:   train/train_steps_per_second 0.082
wandb: 
wandb: 🚀 View run classic-sun-5 at: https://wandb.ai/1249221036/huggingface/runs/bvqzymj7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231001_094800-bvqzymj7/logs
