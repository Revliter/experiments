# load with the lora-finetuned weight, fix it, then only train the classifier


Training Alpaca-LoRA model with params:
gla: True
====================
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
layerwise: True
train_layers: [[28], [29], [30], [31]]
lr_scaling: [1.0, 1.0, 1.0, 1.0]
====================
base_model: decapoda-research/llama-7b-hf
data_path: yahma/alpaca-cleaned
output_dir: ../ckpts/lora_baseline
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca

Some weights of LlamaForCausalLM_GLA were not initialized from the model checkpoint at decapoda-research/llama-7b-hf and are newly initialized: ['gla_norms.0.weight', 'gla_heads.0.weight', 'gla_norms.1.weight', 'gla_heads.1.weight', 'gla_norms.2.weight', 'gla_heads.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 25.48it/s]
Loading cached split indices for dataset at /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow
trainable params: 524,304,384 || all params: 7,148,421,120 || trainable%: 7.334548079898236
Trainable parameters: ['base_model.model.model.norm.weight', 'base_model.model.lm_head.weight', 'base_model.model.gla_heads.0.weight', 'base_model.model.gla_heads.1.weight', 'base_model.model.gla_heads.2.weight', 'base_model.model.gla_norms.0.weight', 'base_model.model.gla_norms.1.weight', 'base_model.model.gla_norms.2.weight']

                                                                
wandb: Currently logged in as: 1249221036. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /scratch/users/nus/e0917621/zangwei/chenting/LLaMA-GLa/wandb/run-20231001_142701-pjqmbqvb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-wind-7
wandb: ⭐️ View project at https://wandb.ai/1249221036/huggingface
wandb: 🚀 View run at https://wandb.ai/1249221036/huggingface/runs/pjqmbqvb

  0%|          | 0/1164 [00:00<?, ?it/s]/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

 17%|█▋        | 200/1164 [32:34<2:36:50,  9.76s/it]
Loss per layer: [11.09656810760498, 11.061325073242188, 11.069990158081055, 1.83214271068573], Total loss: 35.06002426147461
Loss per layer: [10.633252143859863, 10.457171440124512, 10.505864143371582, 1.5395957231521606], Total loss: 33.13588333129883
Loss per layer: [9.727272987365723, 9.693824768066406, 9.637255668640137, 1.215963363647461], Total loss: 30.274316787719727
Loss per layer: [8.60550594329834, 8.43396282196045, 8.58297348022461, 1.3712387084960938], Total loss: 26.993680953979492
Loss per layer: [6.725793838500977, 6.412487506866455, 6.530116081237793, 1.0788531303405762], Total loss: 20.747249603271484
Loss per layer: [6.154867649078369, 6.030001640319824, 6.084139823913574, 1.0714462995529175], Total loss: 19.3404541015625
Loss per layer: [4.038735866546631, 3.802431583404541, 3.833561658859253, 0.933491051197052], Total loss: 12.608220100402832
Loss per layer: [3.501289129257202, 3.4457428455352783, 3.453482151031494, 1.1615653038024902], Total loss: 11.562080383300781
Loss per layer: [3.0494730472564697, 2.952392339706421, 2.9792120456695557, 1.038048505783081], Total loss: 10.019125938415527
Loss per layer: [2.1777453422546387, 2.18861722946167, 2.134539842605591, 0.9361146688461304], Total loss: 7.43701696395874
Loss per layer: [2.1162326335906982, 2.1183364391326904, 2.0791115760803223, 1.2709119319915771], Total loss: 7.584592819213867
Loss per layer: [1.8028630018234253, 1.7639225721359253, 1.7311311960220337, 0.8949928283691406], Total loss: 6.1929097175598145
Loss per layer: [1.982895016670227, 2.024923086166382, 1.9842711687088013, 0.9038554430007935], Total loss: 6.895944595336914
Loss per layer: [1.4438986778259277, 1.3916096687316895, 1.2862366437911987, 0.7876994609832764], Total loss: 4.909444808959961
Loss per layer: [1.8829658031463623, 1.87709641456604, 1.8391190767288208, 1.065493106842041], Total loss: 6.664674282073975
Loss per layer: [1.9442732334136963, 1.9062334299087524, 1.8380008935928345, 1.244550347328186], Total loss: 6.93305778503418
Loss per layer: [1.648653268814087, 1.6113014221191406, 1.5820482969284058, 1.006914734840393], Total loss: 5.848917484283447
Loss per layer: [1.3994437456130981, 1.3440457582473755, 1.3003594875335693, 0.8618273735046387], Total loss: 4.905676364898682
Loss per layer: [1.6086989641189575, 1.6168129444122314, 1.590192437171936, 1.1075931787490845], Total loss: 5.92329740524292
Loss per layer: [1.4559420347213745, 1.3926349878311157, 1.4082404375076294, 0.97364741563797], Total loss: 5.230464935302734


 34%|███▍      | 400/1164 [1:08:00<2:35:54, 12.24s/it]
 {'eval_loss': 0.9842842221260071, 'eval_runtime': 100.3133, 'eval_samples_per_second': 19.938, 'eval_steps_per_second': 2.492, 'epoch': 0.51}
Loss per layer: [1.181990146636963, 1.199566125869751, 1.1681088209152222, 0.9033930897712708], Total loss: 4.453058242797852
Loss per layer: [1.3299851417541504, 1.3216845989227295, 1.2997920513153076, 1.0413501262664795], Total loss: 4.992812156677246
Loss per layer: [1.4053997993469238, 1.3283711671829224, 1.370605707168579, 0.9794923067092896], Total loss: 5.083868980407715
Loss per layer: [1.4581804275512695, 1.4049304723739624, 1.3505417108535767, 0.9823579788208008], Total loss: 5.196010589599609
Loss per layer: [1.4340523481369019, 1.396508812904358, 1.352648138999939, 0.8693531155586243], Total loss: 5.052562713623047
Loss per layer: [1.1367167234420776, 1.1177010536193848, 1.0581635236740112, 0.8343641757965088], Total loss: 4.146945953369141
Loss per layer: [1.807861328125, 1.7326701879501343, 1.7170414924621582, 1.1639569997787476], Total loss: 6.421530246734619
Loss per layer: [1.2693674564361572, 1.2719002962112427, 1.2102950811386108, 0.8945469260215759], Total loss: 4.6461100578308105
Loss per layer: [1.3100130558013916, 1.26211678981781, 1.2568359375, 0.9641820788383484], Total loss: 4.793147563934326
Loss per layer: [1.1053588390350342, 1.1324102878570557, 1.0787620544433594, 0.8832499980926514], Total loss: 4.19978141784668
Loss per layer: [1.1904385089874268, 1.1565676927566528, 1.1365371942520142, 0.904805064201355], Total loss: 4.388348579406738
Loss per layer: [1.4046542644500732, 1.381742238998413, 1.3792845010757446, 1.1351741552352905], Total loss: 5.3008551597595215
Loss per layer: [0.9559508562088013, 0.9543277621269226, 0.9044352173805237, 0.7359380125999451], Total loss: 3.550651788711548
Loss per layer: [1.584619402885437, 1.5586652755737305, 1.5227985382080078, 1.112335443496704], Total loss: 5.77841854095459
Loss per layer: [1.2312837839126587, 1.2257938385009766, 1.1595793962478638, 0.8897414207458496], Total loss: 4.5063982009887695
Loss per layer: [1.0644513368606567, 1.032033920288086, 1.0117162466049194, 0.7240084409713745], Total loss: 3.832210063934326
Loss per layer: [1.3029893636703491, 1.2942323684692383, 1.2675859928131104, 1.0361672639846802], Total loss: 4.900975227355957
Loss per layer: [1.3204846382141113, 1.2903130054473877, 1.268561840057373, 0.9561753869056702], Total loss: 4.835535049438477
Loss per layer: [1.2126033306121826, 1.1567236185073853, 1.1834067106246948, 0.9109958410263062], Total loss: 4.4637298583984375
Loss per layer: [1.082245111465454, 1.0403839349746704, 1.0082038640975952, 0.7411502003669739], Total loss: 3.871983289718628

 52%|█████▏    | 600/1164 [1:47:46<1:47:02, 11.39s/it]
 {'eval_loss': 0.9532868266105652, 'eval_runtime': 100.3512, 'eval_samples_per_second': 19.93, 'eval_steps_per_second': 2.491, 'epoch': 1.03}
Loss per layer: [1.0058033466339111, 1.0308666229248047, 0.9933946132659912, 0.7652014493942261], Total loss: 3.7952661514282227
Loss per layer: [0.9966293573379517, 0.9367677569389343, 0.9202447533607483, 0.7425852417945862], Total loss: 3.596226930618286
Loss per layer: [0.858652651309967, 0.8510854244232178, 0.8178111910820007, 0.7411019206047058], Total loss: 3.268651247024536
Loss per layer: [0.939096987247467, 0.9568656086921692, 0.9135484099388123, 0.8670461177825928], Total loss: 3.6765570640563965
Loss per layer: [1.1302636861801147, 1.1072698831558228, 1.0962058305740356, 0.8737677335739136], Total loss: 4.207507133483887
Loss per layer: [0.9158249497413635, 0.8926774859428406, 0.9156058430671692, 0.7352160811424255], Total loss: 3.459324359893799
Loss per layer: [1.0464719533920288, 1.0363551378250122, 1.021278977394104, 0.8914139866828918], Total loss: 3.9955198764801025
Loss per layer: [1.1384450197219849, 1.0968034267425537, 1.103114366531372, 0.9249942302703857], Total loss: 4.263357162475586
Loss per layer: [1.071189284324646, 1.0869228839874268, 1.070138692855835, 0.9374009370803833], Total loss: 4.165651798248291
Loss per layer: [1.0256600379943848, 1.0077869892120361, 1.0261542797088623, 0.8746671080589294], Total loss: 3.9342684745788574
{'loss': 1.0136, 'learning_rate': 0.00018721804511278195, 'epoch': 1.29}
Loss per layer: [0.9458940625190735, 0.9016037583351135, 0.9019854664802551, 0.7807855606079102], Total loss: 3.530268907546997
Loss per layer: [1.0168455839157104, 0.9951168894767761, 0.9695439338684082, 0.7556759119033813], Total loss: 3.737182140350342
Loss per layer: [1.0506274700164795, 1.0380257368087769, 1.0226011276245117, 0.8602180480957031], Total loss: 3.9714722633361816
Loss per layer: [1.0459061861038208, 1.0353237390518188, 1.021500587463379, 0.8730775117874146], Total loss: 3.9758081436157227
Loss per layer: [0.9038454294204712, 0.9023385047912598, 0.8751465678215027, 0.7306397557258606], Total loss: 3.4119701385498047
Loss per layer: [1.075262188911438, 1.0463528633117676, 1.0533729791641235, 0.8656362295150757], Total loss: 4.040624141693115
Loss per layer: [1.0173842906951904, 0.9365994930267334, 0.9510108232498169, 0.6954945921897888], Total loss: 3.6004891395568848
Loss per layer: [0.9826074242591858, 0.9629063010215759, 0.9480040669441223, 0.7947671413421631], Total loss: 3.6882848739624023
Loss per layer: [0.8407694101333618, 0.8273441195487976, 0.8067809343338013, 0.6493980288505554], Total loss: 3.1242926120758057
Loss per layer: [1.1607837677001953, 1.1648036241531372, 1.13787841796875, 0.9054591655731201], Total loss: 4.368925094604492

 69%|██████▊   | 800/1164 [2:26:00<1:03:09, 10.41s/it]
 {'eval_loss': 0.9393441677093506, 'eval_runtime': 100.1226, 'eval_samples_per_second': 19.976, 'eval_steps_per_second': 2.497, 'epoch': 1.54}
Loss per layer: [1.4944229125976562, 1.4477660655975342, 1.4680243730545044, 1.0781301259994507], Total loss: 5.488343715667725
Loss per layer: [0.9810305833816528, 0.9646194577217102, 0.9527899622917175, 0.7996121048927307], Total loss: 3.698052167892456
Loss per layer: [0.7650232911109924, 0.7551199197769165, 0.7292768359184265, 0.6563424468040466], Total loss: 2.9057626724243164
Loss per layer: [1.1638544797897339, 1.1520742177963257, 1.1230765581130981, 0.9388381242752075], Total loss: 4.377843379974365
Loss per layer: [1.089743733406067, 1.0651113986968994, 1.05763840675354, 0.8567287921905518], Total loss: 4.069222450256348
Loss per layer: [0.997505784034729, 0.9705830812454224, 0.9488255977630615, 0.8047902584075928], Total loss: 3.7217047214508057
Loss per layer: [1.2617299556732178, 1.2562066316604614, 1.2233378887176514, 0.8659204840660095], Total loss: 4.607194900512695
Loss per layer: [0.9985113143920898, 0.9954510927200317, 0.974088191986084, 0.8760992884635925], Total loss: 3.8441498279571533
Loss per layer: [1.022905707359314, 1.010838508605957, 1.010326623916626, 0.8918902277946472], Total loss: 3.9359612464904785
Loss per layer: [1.2119168043136597, 1.2176145315170288, 1.172499418258667, 0.8717918992042542], Total loss: 4.473822593688965
Loss per layer: [0.9085831642150879, 0.9159109592437744, 0.8920888900756836, 0.8015962839126587], Total loss: 3.518179416656494
Loss per layer: [0.7813928723335266, 0.7916157245635986, 0.7870036363601685, 0.665935218334198], Total loss: 3.025947332382202
Loss per layer: [0.9777271151542664, 0.9473145604133606, 0.9885442852973938, 0.7841520309448242], Total loss: 3.6977379322052
Loss per layer: [0.9660558104515076, 0.9304975867271423, 0.9128314852714539, 0.7655010223388672], Total loss: 3.574885845184326
Loss per layer: [1.0090503692626953, 0.9952004551887512, 0.9953584671020508, 0.803167998790741], Total loss: 3.8027772903442383
Loss per layer: [0.8737806677818298, 0.8531385064125061, 0.868347704410553, 0.7252277731895447], Total loss: 3.3204946517944336
Loss per layer: [0.9521023035049438, 0.9186299443244934, 0.9103595018386841, 0.6905435919761658], Total loss: 3.471635341644287
Loss per layer: [0.8546526432037354, 0.8286842703819275, 0.8295650482177734, 0.6608755588531494], Total loss: 3.1737775802612305
Loss per layer: [1.0478192567825317, 1.0310471057891846, 0.9734796285629272, 0.8183166980743408], Total loss: 3.8706629276275635
Loss per layer: [0.9631711840629578, 0.9298350214958191, 0.9152119159698486, 0.753037691116333], Total loss: 3.561255693435669

          

 86%|████████▌ | 1000/1164 [3:02:43<28:11, 10.32s/it]
 {'eval_loss': 0.9212834239006042, 'eval_runtime': 100.1544, 'eval_samples_per_second': 19.969, 'eval_steps_per_second': 2.496, 'epoch': 2.06}
Loss per layer: [1.116748332977295, 1.0749715566635132, 1.0530455112457275, 0.8329915404319763], Total loss: 4.077756881713867
Loss per layer: [0.9064998030662537, 0.8818519711494446, 0.8795923590660095, 0.6846657395362854], Total loss: 3.352609872817993
Loss per layer: [0.6788785457611084, 0.6691128611564636, 0.6459397077560425, 0.5347095727920532], Total loss: 2.5286407470703125
Loss per layer: [0.8016546368598938, 0.7941131591796875, 0.7875587940216064, 0.6381922364234924], Total loss: 3.0215187072753906
Loss per layer: [0.8354372978210449, 0.8108503222465515, 0.8038902878761292, 0.6457678079605103], Total loss: 3.0959458351135254
Loss per layer: [0.8135113716125488, 0.7929813265800476, 0.8023618459701538, 0.6681957840919495], Total loss: 3.07705020904541
Loss per layer: [1.2969931364059448, 1.2729790210723877, 1.2600467205047607, 1.0370843410491943], Total loss: 4.867103099822998
Loss per layer: [1.1669082641601562, 1.1557490825653076, 1.134315013885498, 0.9092100858688354], Total loss: 4.366182327270508
Loss per layer: [0.9565196633338928, 0.9536945223808289, 0.9665462970733643, 0.8359255194664001], Total loss: 3.712686061859131
Loss per layer: [0.9820267558097839, 0.9494890570640564, 0.958429217338562, 0.8291704654693604], Total loss: 3.7191154956817627
Loss per layer: [0.7792182564735413, 0.7712817788124084, 0.7625947594642639, 0.617851972579956], Total loss: 2.9309468269348145
Loss per layer: [1.1166627407073975, 1.0956521034240723, 1.0442699193954468, 0.8500206470489502], Total loss: 4.106605529785156
Loss per layer: [0.9136223793029785, 0.8877789974212646, 0.8846385478973389, 0.6121931076049805], Total loss: 3.2982330322265625
Loss per layer: [0.9268925189971924, 0.8970047831535339, 0.9069958925247192, 0.7309995889663696], Total loss: 3.461892604827881
Loss per layer: [1.0686981678009033, 1.0627912282943726, 1.0601791143417358, 0.7260686159133911], Total loss: 3.9177370071411133
Loss per layer: [0.9528157711029053, 0.9321702122688293, 0.9065257906913757, 0.7361741065979004], Total loss: 3.5276858806610107
Loss per layer: [1.0593934059143066, 1.0330877304077148, 1.0179437398910522, 0.7294255495071411], Total loss: 3.839850425720215
Loss per layer: [1.099879264831543, 1.069378137588501, 1.0699553489685059, 0.8373445868492126], Total loss: 4.076557159423828
Loss per layer: [0.9698484539985657, 0.9676232933998108, 0.9577506184577942, 0.7529450058937073], Total loss: 3.648167371749878
Loss per layer: [0.7913588881492615, 0.8306238055229187, 0.8151865005493164, 0.6933609247207642], Total loss: 3.1305298805236816
{'loss': 0.7856, 'learning_rate': 4.624060150375939e-05, 'epoch': 2.57}

100%|██████████| 1164/1164 [3:32:18<00:00, 10.94s/it]
{'eval_loss': 0.9120976328849792, 'eval_runtime': 100.2648, 'eval_samples_per_second': 19.947, 'eval_steps_per_second': 2.493, 'epoch': 2.57}
Loss per layer: [1.1550145149230957, 1.136603593826294, 1.1359646320343018, 0.9107223153114319], Total loss: 4.3383049964904785
Loss per layer: [1.121368646621704, 1.0710877180099487, 1.0433435440063477, 0.8025107383728027], Total loss: 4.038310527801514
Loss per layer: [0.9575944542884827, 0.8947937488555908, 0.8495757579803467, 0.7303789854049683], Total loss: 3.432343006134033
Loss per layer: [1.097796082496643, 1.0937508344650269, 1.0556371212005615, 0.808315634727478], Total loss: 4.05549955368042
Loss per layer: [0.7167875170707703, 0.7336728572845459, 0.7339172959327698, 0.5817739963531494], Total loss: 2.7661516666412354
Loss per layer: [1.1960045099258423, 1.1590087413787842, 1.148245096206665, 0.9099656343460083], Total loss: 4.413224220275879
Loss per layer: [0.8006845116615295, 0.770863950252533, 0.7760294079780579, 0.6251431107521057], Total loss: 2.9727208614349365
Loss per layer: [1.0384546518325806, 1.0412479639053345, 0.992655873298645, 0.7535713911056519], Total loss: 3.825930118560791
Loss per layer: [0.8329792618751526, 0.8311046361923218, 0.8172181248664856, 0.6309500932693481], Total loss: 3.1122522354125977
Loss per layer: [0.9659059643745422, 0.9265134334564209, 0.9560653567314148, 0.7603201270103455], Total loss: 3.608804941177368
Loss per layer: [0.8556432127952576, 0.8633973002433777, 0.8350428938865662, 0.6267638206481934], Total loss: 3.18084716796875
Loss per layer: [0.9409799575805664, 0.938034176826477, 0.9186016321182251, 0.7476733922958374], Total loss: 3.5452890396118164
Loss per layer: [1.0693503618240356, 1.0771493911743164, 1.0545532703399658, 0.9151253700256348], Total loss: 4.116178512573242
Loss per layer: [0.8451220989227295, 0.8417257070541382, 0.8434968590736389, 0.6615756154060364], Total loss: 3.191920280456543
Loss per layer: [1.0282721519470215, 1.0147525072097778, 0.9496765732765198, 0.7846761345863342], Total loss: 3.777377128601074
Loss per layer: [0.8961321711540222, 0.859420657157898, 0.8479470014572144, 0.7085387706756592], Total loss: 3.3120386600494385
{'train_runtime': 12744.6934, 'train_samples_per_second': 11.713, 'train_steps_per_second': 0.091, 'train_loss': 0.8767919442088333, 'epoch': 2.99}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.026 MB of 0.026 MB uploaded (0.000 MB deduped)
wandb: \ 0.026 MB of 0.026 MB uploaded (0.000 MB deduped)
wandb: | 0.026 MB of 0.049 MB uploaded (0.000 MB deduped)
wandb: / 0.049 MB of 0.049 MB uploaded (0.000 MB deduped)
wandb: - 0.049 MB of 0.049 MB uploaded (0.000 MB deduped)
wandb: 
wandb: Run history:
wandb:                      eval/loss █▅▄▂▁
wandb:                   eval/runtime ▇█▁▂▅
wandb:        eval/samples_per_second ▂▁█▇▄
wandb:          eval/steps_per_second ▂▁█▇▃
wandb:                    train/epoch ▁▂▃▄▅▇▇█
wandb:              train/global_step ▁▂▃▄▅▇▇█
wandb:            train/learning_rate █▁
wandb:                     train/loss █▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.9121
wandb:                   eval/runtime 100.2648
wandb:        eval/samples_per_second 19.947
wandb:          eval/steps_per_second 2.493
wandb:                    train/epoch 2.99
wandb:              train/global_step 1164
wandb:            train/learning_rate 5e-05
wandb:                     train/loss 0.7856
wandb:               train/total_flos 1.5432046690015642e+18
wandb:               train/train_loss 0.87679
wandb:            train/train_runtime 12744.6934
wandb: train/train_samples_per_second 11.713
wandb:   train/train_steps_per_second 0.091
wandb: 
wandb: 🚀 View run fancy-wind-7 at: https://wandb.ai/1249221036/huggingface/runs/pjqmbqvb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231001_142701-pjqmbqvb/logs
