# Layerwise Last4 no lr_scaling
# The eval loss is wrong here

Training Alpaca-LoRA model with params:
gla: True
====================
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
layerwise: True
train_layers: [[28], [29], [30], [31]]
lr_scaling: None
====================
base_model: decapoda-research/llama-7b-hf
data_path: yahma/alpaca-cleaned
output_dir: ../ckpts/ly4-alpaca
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:19<00:00,  1.63it/s]

Some weights of LlamaForCausalLM_GLA were not initialized from the model checkpoint at decapoda-research/llama-7b-hf and are newly initialized: ['gla_norms.0.weight', 'gla_heads.2.weight', 'gla_heads.0.weight', 'gla_norms.1.weight', 'gla_heads.1.weight', 'gla_norms.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 70.07it/s]
Loading cached split indices for dataset at /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow
trainable params: 524,828,672 || all params: 7,132,168,192 || trainable%: 7.3586132277235
Trainable parameters: ['base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight', 'base_model.model.gla_heads.0.weight', 'base_model.model.gla_heads.1.weight', 'base_model.model.gla_heads.2.weight', 'base_model.model.gla_norms.0.weight', 'base_model.model.gla_norms.1.weight', 'base_model.model.gla_norms.2.weight']


Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 906.03 examples/s]
                                                                
wandb: Currently logged in as: 1249221036. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /scratch/users/nus/e0917621/zangwei/chenting/LLaMA-GLa/wandb/run-20230929_102350-e2lwthla
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-jazz-4
wandb: â­ï¸ View project at https://wandb.ai/1249221036/huggingface
wandb: ðŸš€ View run at https://wandb.ai/1249221036/huggingface/runs/e2lwthla

  0%|          | 0/1164 [00:00<?, ?it/s]/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

 17%|â–ˆâ–‹        | 200/1164 [36:55<2:57:14, 11.03s/it]
 Loss per layer: [10.969938278198242, 11.166669845581055, 11.045737266540527, 1.6909009218215942], Total loss: 34.87324905395508
Loss per layer: [10.042899131774902, 10.50633716583252, 10.1661958694458, 1.5083389282226562], Total loss: 32.22377014160156
Loss per layer: [9.393232345581055, 9.528746604919434, 9.13611125946045, 1.378095269203186], Total loss: 29.436185836791992
Loss per layer: [7.255964279174805, 7.638520240783691, 7.29026460647583, 0.9882761836051941], Total loss: 23.173025131225586
Loss per layer: [5.476598739624023, 5.297797679901123, 4.989809036254883, 1.0725464820861816], Total loss: 16.83675193786621
Loss per layer: [5.0716352462768555, 4.769810199737549, 4.418076992034912, 1.278885841369629], Total loss: 15.538409233093262
Loss per layer: [4.544783115386963, 4.2643280029296875, 4.225988864898682, 1.206712007522583], Total loss: 14.24181079864502
Loss per layer: [2.801151990890503, 2.634711980819702, 2.626368284225464, 1.1760706901550293], Total loss: 9.238302230834961
Loss per layer: [2.281869411468506, 2.198927164077759, 2.2413361072540283, 1.0511142015457153], Total loss: 7.773246765136719
Loss per layer: [2.140227794647217, 2.094799041748047, 2.0828354358673096, 1.0304349660873413], Total loss: 7.348297595977783
Loss per layer: [1.873661756515503, 1.8049838542938232, 1.7878867387771606, 1.1096022129058838], Total loss: 6.57613468170166
Loss per layer: [1.3347984552383423, 1.32693350315094, 1.2887581586837769, 0.9108889698982239], Total loss: 4.861379146575928
Loss per layer: [1.887641429901123, 1.842550277709961, 1.7896969318389893, 1.0867868661880493], Total loss: 6.606675624847412
Loss per layer: [1.8866939544677734, 1.7615416049957275, 1.7916929721832275, 1.0625451803207397], Total loss: 6.502473831176758
Loss per layer: [1.377659797668457, 1.3555924892425537, 1.2808218002319336, 0.8876826763153076], Total loss: 4.90175724029541
Loss per layer: [1.54831063747406, 1.4784038066864014, 1.4930204153060913, 1.0465039014816284], Total loss: 5.566238880157471
Loss per layer: [2.013010025024414, 1.9271526336669922, 1.9107043743133545, 1.1625120639801025], Total loss: 7.013379096984863
Loss per layer: [1.5984182357788086, 1.5449739694595337, 1.5474144220352173, 1.1794434785842896], Total loss: 5.8702497482299805
Loss per layer: [1.4889241456985474, 1.4112902879714966, 1.3758682012557983, 0.9980331063270569], Total loss: 5.274115562438965
Loss per layer: [1.4316143989562988, 1.3843570947647095, 1.3214170932769775, 0.9874173998832703], Total loss: 5.124805450439453

 34%|â–ˆâ–ˆâ–ˆâ–      | 400/1164 [1:16:38<2:29:59, 11.78s/it]
 {'eval_loss': 5.394028186798096, 'eval_runtime': 95.602, 'eval_samples_per_second': 20.92, 'eval_steps_per_second': 2.615, 'epoch': 0.51}
Loss per layer: [1.6383368968963623, 1.5835752487182617, 1.5754085779190063, 1.2901283502578735], Total loss: 6.087449073791504
Loss per layer: [1.3911892175674438, 1.3598664999008179, 1.321722149848938, 1.005881428718567], Total loss: 5.0786590576171875
Loss per layer: [1.638066291809082, 1.572887897491455, 1.5402530431747437, 1.1163169145584106], Total loss: 5.867524147033691
Loss per layer: [1.323960781097412, 1.2630939483642578, 1.2289386987686157, 0.9896438121795654], Total loss: 4.805637359619141
Loss per layer: [1.4866516590118408, 1.348626971244812, 1.2828645706176758, 0.8346649408340454], Total loss: 4.952807903289795
Loss per layer: [1.3665399551391602, 1.3675403594970703, 1.3068581819534302, 0.9855638146400452], Total loss: 5.0265021324157715
Loss per layer: [1.3152830600738525, 1.289972186088562, 1.209175705909729, 0.8420348167419434], Total loss: 4.656465530395508
Loss per layer: [1.6224658489227295, 1.5233029127120972, 1.45225191116333, 1.0338443517684937], Total loss: 5.63186502456665
Loss per layer: [1.6179426908493042, 1.6014199256896973, 1.543637990951538, 1.1920437812805176], Total loss: 5.955044269561768
Loss per layer: [1.3618301153182983, 1.3009008169174194, 1.261573314666748, 0.9644875526428223], Total loss: 4.888792037963867
Loss per layer: [1.1400854587554932, 1.1085025072097778, 1.1098320484161377, 0.9199768900871277], Total loss: 4.278397083282471
Loss per layer: [1.4641571044921875, 1.4251928329467773, 1.383875846862793, 1.1016064882278442], Total loss: 5.3748321533203125
Loss per layer: [1.4122962951660156, 1.3533580303192139, 1.365958571434021, 0.954883337020874], Total loss: 5.086496353149414
Loss per layer: [1.4829204082489014, 1.4756892919540405, 1.408995270729065, 1.1136735677719116], Total loss: 5.481278419494629
Loss per layer: [1.3804311752319336, 1.376556158065796, 1.3551992177963257, 1.076117992401123], Total loss: 5.188304424285889
Loss per layer: [1.1792638301849365, 1.130855917930603, 1.1094341278076172, 0.915745198726654], Total loss: 4.335299015045166
Loss per layer: [1.2282204627990723, 1.2026885747909546, 1.1484484672546387, 1.0198413133621216], Total loss: 4.599198818206787
Loss per layer: [1.1258753538131714, 1.0771223306655884, 1.046460747718811, 0.8533632159233093], Total loss: 4.102821350097656
Loss per layer: [1.2941458225250244, 1.2828848361968994, 1.247232437133789, 1.038779616355896], Total loss: 4.863042831420898
Loss per layer: [1.1846857070922852, 1.1265288591384888, 1.1321821212768555, 0.9722431898117065], Total loss: 4.415639877319336


 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 600/1164 [1:57:19<1:49:14, 11.62s/it]
 {'eval_loss': 4.684913158416748, 'eval_runtime': 95.9128, 'eval_samples_per_second': 20.852, 'eval_steps_per_second': 2.607, 'epoch': 1.03}
Loss per layer: [1.1269859075546265, 1.0955712795257568, 1.0884292125701904, 0.934984028339386], Total loss: 4.245970249176025
Loss per layer: [1.1110234260559082, 1.1408501863479614, 1.118099570274353, 0.8670461177825928], Total loss: 4.2370195388793945
Loss per layer: [1.0248051881790161, 0.9642037749290466, 0.9506685733795166, 0.7461109757423401], Total loss: 3.68578839302063
Loss per layer: [1.024594783782959, 0.9972712397575378, 0.9877219200134277, 0.7602387070655823], Total loss: 3.769826650619507
Loss per layer: [1.0486233234405518, 0.9955995082855225, 1.004472255706787, 0.7689223885536194], Total loss: 3.817617416381836
Loss per layer: [1.0284329652786255, 1.0042401552200317, 1.0271382331848145, 0.7819310426712036], Total loss: 3.841742515563965
Loss per layer: [1.080606460571289, 1.0691627264022827, 1.0538324117660522, 0.9157237410545349], Total loss: 4.119325637817383
Loss per layer: [1.049836277961731, 1.0255621671676636, 0.9899904727935791, 0.8501679301261902], Total loss: 3.9155569076538086
Loss per layer: [1.1841098070144653, 1.134295105934143, 1.1208441257476807, 0.9044562578201294], Total loss: 4.343705177307129
Loss per layer: [1.2004574537277222, 1.1880133152008057, 1.2011528015136719, 1.0168744325637817], Total loss: 4.606497764587402
{'loss': 1.0187, 'learning_rate': 0.00018721804511278195, 'epoch': 1.29}
Loss per layer: [0.8058058619499207, 0.776919960975647, 0.7525801062583923, 0.6075156927108765], Total loss: 2.942821502685547
Loss per layer: [1.0684338808059692, 1.0442075729370117, 0.9901143908500671, 0.862877607345581], Total loss: 3.9656333923339844
Loss per layer: [1.1955372095108032, 1.1570262908935547, 1.1208226680755615, 0.8877469897270203], Total loss: 4.361133098602295
Loss per layer: [1.0822700262069702, 1.0816093683242798, 1.064389705657959, 0.8851296305656433], Total loss: 4.113398551940918
Loss per layer: [0.939912736415863, 0.882880449295044, 0.8969002366065979, 0.775945246219635], Total loss: 3.495638608932495
Loss per layer: [1.0690271854400635, 1.053171157836914, 1.0124856233596802, 0.8884783387184143], Total loss: 4.023162364959717
Loss per layer: [1.145658254623413, 1.1316207647323608, 1.1077337265014648, 0.8810989856719971], Total loss: 4.266111373901367
Loss per layer: [0.9910666346549988, 0.9707552790641785, 0.9593963623046875, 0.7402104735374451], Total loss: 3.661428928375244
Loss per layer: [0.9506283402442932, 0.9670150876045227, 0.9461215734481812, 0.823647677898407], Total loss: 3.687412738800049
Loss per layer: [1.2552680969238281, 1.1957991123199463, 1.1890426874160767, 0.9461297392845154], Total loss: 4.586239814758301


 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 800/1164 [2:36:59<1:07:12, 11.08s/it]
 {'eval_loss': 4.511772632598877, 'eval_runtime': 95.9529, 'eval_samples_per_second': 20.844, 'eval_steps_per_second': 2.605, 'epoch': 1.54}
Loss per layer: [1.1834605932235718, 1.1647522449493408, 1.1411826610565186, 1.0174752473831177], Total loss: 4.506870746612549
Loss per layer: [1.308388590812683, 1.2709506750106812, 1.252511978149414, 1.0899946689605713], Total loss: 4.92184591293335
Loss per layer: [1.0330162048339844, 0.9829249978065491, 0.9503506422042847, 0.8229947686195374], Total loss: 3.7892866134643555
Loss per layer: [1.120502233505249, 1.0580600500106812, 1.0371084213256836, 0.8185116648674011], Total loss: 4.034182071685791
Loss per layer: [0.9390227198600769, 0.9307620525360107, 0.8961920142173767, 0.7732735276222229], Total loss: 3.539250373840332
Loss per layer: [0.796114981174469, 0.7691041231155396, 0.746963620185852, 0.6873815059661865], Total loss: 2.9995644092559814
Loss per layer: [1.0083149671554565, 0.9785699844360352, 0.9730769991874695, 0.7975950241088867], Total loss: 3.757556915283203
Loss per layer: [1.0254104137420654, 0.997897207736969, 0.9766237139701843, 0.8129894733428955], Total loss: 3.8129208087921143
Loss per layer: [0.9461368918418884, 0.9247410893440247, 0.8869699835777283, 0.7632499933242798], Total loss: 3.5210981369018555
Loss per layer: [1.339140772819519, 1.2943289279937744, 1.2398239374160767, 1.0608488321304321], Total loss: 4.934142112731934
Loss per layer: [1.0443897247314453, 1.0138970613479614, 0.9776782393455505, 0.7232638597488403], Total loss: 3.7592287063598633
Loss per layer: [0.8716884851455688, 0.8533083200454712, 0.8052005767822266, 0.6588206887245178], Total loss: 3.1890180110931396
Loss per layer: [1.20952308177948, 1.1699680089950562, 1.158376693725586, 0.9428074955940247], Total loss: 4.480675220489502
Loss per layer: [1.0051209926605225, 0.9932300448417664, 0.970616340637207, 0.7621141076087952], Total loss: 3.731081485748291
Loss per layer: [1.0548988580703735, 1.0311568975448608, 1.041020393371582, 0.8569582104682922], Total loss: 3.984034299850464
Loss per layer: [1.0112431049346924, 1.0056304931640625, 0.9874234795570374, 0.7971500754356384], Total loss: 3.8014471530914307
Loss per layer: [0.9693368077278137, 0.9629865884780884, 0.9060867428779602, 0.7759428024291992], Total loss: 3.6143529415130615
Loss per layer: [1.1388925313949585, 1.0955026149749756, 1.0448130369186401, 0.8338555097579956], Total loss: 4.113063812255859
Loss per layer: [0.8967311978340149, 0.8510985374450684, 0.8448090553283691, 0.6331847310066223], Total loss: 3.225823402404785
Loss per layer: [0.9132431745529175, 0.9238902926445007, 0.8915769457817078, 0.7397574782371521], Total loss: 3.468467950820923


 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1000/1164 [3:15:32<30:01, 10.98s/it]
 {'eval_loss': 4.413780212402344, 'eval_runtime': 95.7032, 'eval_samples_per_second': 20.898, 'eval_steps_per_second': 2.612, 'epoch': 2.06}
Loss per layer: [0.8466125726699829, 0.8523358702659607, 0.8256098031997681, 0.727189838886261], Total loss: 3.2517478466033936
Loss per layer: [1.0631780624389648, 1.021859884262085, 1.0368499755859375, 0.867279589176178], Total loss: 3.9891674518585205
Loss per layer: [0.800825834274292, 0.7752126455307007, 0.7501842379570007, 0.661856472492218], Total loss: 2.988079071044922
Loss per layer: [1.0689924955368042, 1.0137109756469727, 0.9938725233078003, 0.8401163816452026], Total loss: 3.9166927337646484
Loss per layer: [1.2885292768478394, 1.2659927606582642, 1.23857581615448, 1.0929419994354248], Total loss: 4.886039733886719
Loss per layer: [0.859053909778595, 0.8825789093971252, 0.839845597743988, 0.7505240440368652], Total loss: 3.3320024013519287
Loss per layer: [1.1606544256210327, 1.175027847290039, 1.1375844478607178, 0.9929103851318359], Total loss: 4.466176986694336
Loss per layer: [1.114772081375122, 1.1086416244506836, 1.0965503454208374, 0.871228814125061], Total loss: 4.191192626953125
Loss per layer: [0.9755944609642029, 0.9741044640541077, 0.9413970112800598, 0.8645445108413696], Total loss: 3.7556405067443848
Loss per layer: [0.8457066416740417, 0.8279250264167786, 0.8050352334976196, 0.702357292175293], Total loss: 3.1810240745544434
Loss per layer: [0.7509288191795349, 0.7656667828559875, 0.7330617308616638, 0.5659321546554565], Total loss: 2.815589427947998
Loss per layer: [0.8369911909103394, 0.7937927842140198, 0.7820378541946411, 0.6644625067710876], Total loss: 3.077284336090088
Loss per layer: [1.0223286151885986, 0.9960485696792603, 0.9853935837745667, 0.8002166152000427], Total loss: 3.803987503051758
Loss per layer: [1.0882744789123535, 1.0648441314697266, 1.0436606407165527, 0.8829298615455627], Total loss: 4.079709053039551
Loss per layer: [1.1314517259597778, 1.107014775276184, 1.0745664834976196, 0.91288161277771], Total loss: 4.22591495513916
Loss per layer: [0.8051363825798035, 0.8052170276641846, 0.8111855983734131, 0.6851657032966614], Total loss: 3.1067047119140625
Loss per layer: [0.9718029499053955, 0.9526891708374023, 0.9070864319801331, 0.8064051270484924], Total loss: 3.637983560562134
Loss per layer: [0.9395586252212524, 0.905250072479248, 0.8799673914909363, 0.8119903206825256], Total loss: 3.536766290664673
Loss per layer: [0.8760061264038086, 0.8221792578697205, 0.8158840537071228, 0.7073770761489868], Total loss: 3.2214465141296387
Loss per layer: [0.9010745286941528, 0.8405154943466187, 0.8558291792869568, 0.6910698413848877], Total loss: 3.2884891033172607
{'loss': 0.809, 'learning_rate': 4.624060150375939e-05, 'epoch': 2.57}

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1164/1164 [3:48:13<00:00, 11.76s/it]
{'eval_loss': 4.370448112487793, 'eval_runtime': 95.7198, 'eval_samples_per_second': 20.894, 'eval_steps_per_second': 2.612, 'epoch': 2.57}
Loss per layer: [1.1603583097457886, 1.118706464767456, 1.126624345779419, 0.9606741070747375], Total loss: 4.366363048553467
Loss per layer: [1.0702003240585327, 1.0470199584960938, 1.0424457788467407, 0.900610625743866], Total loss: 4.060276508331299
Loss per layer: [0.9939700365066528, 0.9818120002746582, 0.9641801118850708, 0.8799172043800354], Total loss: 3.8198792934417725
Loss per layer: [0.651179313659668, 0.6527252793312073, 0.6484439969062805, 0.5965043306350708], Total loss: 2.5488529205322266
Loss per layer: [0.9043606519699097, 0.8669231534004211, 0.8723522424697876, 0.7597804069519043], Total loss: 3.403416633605957
Loss per layer: [0.9258050918579102, 0.89021235704422, 0.9002213478088379, 0.7290985584259033], Total loss: 3.4453372955322266
Loss per layer: [0.9591121077537537, 0.9421772360801697, 0.9264923930168152, 0.8327910900115967], Total loss: 3.6605727672576904
Loss per layer: [1.2057063579559326, 1.1952381134033203, 1.1595416069030762, 1.0176019668579102], Total loss: 4.57808780670166
Loss per layer: [0.9230352640151978, 0.9362849593162537, 0.8922967910766602, 0.7371222972869873], Total loss: 3.488739252090454
Loss per layer: [1.116767168045044, 1.1042606830596924, 1.0944945812225342, 0.8961555361747742], Total loss: 4.2116780281066895
Loss per layer: [0.8721221685409546, 0.8155960440635681, 0.809887170791626, 0.7191207408905029], Total loss: 3.216726064682007
Loss per layer: [0.9802479147911072, 0.9712516665458679, 0.9255942106246948, 0.7287406921386719], Total loss: 3.605834484100342
Loss per layer: [1.0443143844604492, 1.0628671646118164, 1.0166209936141968, 0.8850958347320557], Total loss: 4.008898735046387
Loss per layer: [1.393072247505188, 1.3585543632507324, 1.3387852907180786, 1.152171015739441], Total loss: 5.24258279800415
Loss per layer: [1.0779188871383667, 1.0431435108184814, 1.0286126136779785, 0.888821542263031], Total loss: 4.038496494293213
Loss per layer: [0.8418886065483093, 0.8154878616333008, 0.8049221634864807, 0.7191667556762695], Total loss: 3.1814653873443604
{'train_runtime': 13697.7504, 'train_samples_per_second': 10.898, 'train_steps_per_second': 0.085, 'train_loss': 0.8923623979706126, 'epoch': 2.99}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss â–ˆâ–ƒâ–‚â–â–
wandb:                   eval/runtime â–â–‡â–ˆâ–ƒâ–ƒ
wandb:        eval/samples_per_second â–ˆâ–‚â–â–†â–†
wandb:          eval/steps_per_second â–ˆâ–‚â–â–†â–†
wandb:                    train/epoch â–â–‚â–ƒâ–„â–…â–‡â–‡â–ˆ
wandb:              train/global_step â–â–‚â–ƒâ–„â–…â–‡â–‡â–ˆ
wandb:            train/learning_rate â–ˆâ–
wandb:                     train/loss â–ˆâ–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                      eval/loss 4.37045
wandb:                   eval/runtime 95.7198
wandb:        eval/samples_per_second 20.894
wandb:          eval/steps_per_second 2.612
wandb:                    train/epoch 2.99
wandb:              train/global_step 1164
wandb:            train/learning_rate 5e-05
wandb:                     train/loss 0.809
wandb:               train/total_flos 1.5401130141753016e+18
wandb:               train/train_loss 0.89236
wandb:            train/train_runtime 13697.7504
wandb: train/train_samples_per_second 10.898
wandb:   train/train_steps_per_second 0.085
wandb: 
wandb: ðŸš€ View run icy-jazz-4 at: https://wandb.ai/1249221036/huggingface/runs/e2lwthla
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230929_102350-e2lwthla/logs
