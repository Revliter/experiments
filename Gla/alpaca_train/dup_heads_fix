# duplicate the last lm_head and norm to each block, then fix them and train lora (fix the bugs in the previous version)

Training Alpaca-LoRA model with params:
gla: True
====================
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
layerwise: True
train_layers: [[28], [29], [30], [31]]
lr_scaling: [1.0, 1.0, 1.0, 1.0]
====================
base_model: decapoda-research/llama-7b-hf
data_path: yahma/alpaca-cleaned
output_dir: ../ckpts/debug
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca


Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:51<00:00,  1.57s/it]
Some weights of LlamaForCausalLM_GLA were not initialized from the model checkpoint at decapoda-research/llama-7b-hf and are newly initialized: ['gla_norms.1.weight', 'gla_heads.1.weight', 'gla_heads.0.weight', 'gla_heads.2.weight', 'gla_norms.0.weight', 'gla_norms.2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 67.85it/s]
Loading cached split indices for dataset at /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow
trainable params: 524,288 || all params: 7,132,168,192 || trainable%: 0.0073510324754831585
Trainable parameters: ['base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight']

                                                                  

Map:   0%|          | 0/2000 [00:00<?, ? examples/s]
Map:   5%|▍         | 97/2000 [00:00<00:01, 956.21 examples/s]
Map:  10%|█         | 204/2000 [00:00<00:01, 1014.34 examples/s]
Map:  16%|█▌        | 317/2000 [00:00<00:01, 1064.94 examples/s]
Map:  23%|██▎       | 460/2000 [00:00<00:01, 1007.23 examples/s]
Map:  31%|███       | 614/2000 [00:00<00:01, 1006.66 examples/s]
Map:  38%|███▊      | 755/2000 [00:00<00:01, 976.28 examples/s] 
Map:  43%|████▎     | 854/2000 [00:00<00:01, 973.89 examples/s]
Map:  50%|████▉     | 997/2000 [00:01<00:01, 961.89 examples/s]
Map:  55%|█████▌    | 1104/2000 [00:01<00:01, 874.49 examples/s]
Map:  60%|██████    | 1206/2000 [00:01<00:00, 906.39 examples/s]
Map:  65%|██████▌   | 1308/2000 [00:01<00:00, 931.98 examples/s]
Map:  70%|███████   | 1409/2000 [00:01<00:00, 951.26 examples/s]
Map:  76%|███████▌  | 1511/2000 [00:01<00:00, 967.04 examples/s]
Map:  83%|████████▎ | 1664/2000 [00:01<00:00, 981.01 examples/s]
Map:  89%|████████▊ | 1774/2000 [00:01<00:00, 1008.78 examples/s]
Map:  96%|█████████▌| 1916/2000 [00:01<00:00, 981.46 examples/s] 
                                                                
wandb: Currently logged in as: 1249221036. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /scratch/users/nus/e0917621/zangwei/chenting/LLaMA-GLa/wandb/run-20231006_124016-xq5ajxm0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-capybara-18
wandb: ⭐️ View project at https://wandb.ai/1249221036/huggingface
wandb: 🚀 View run at https://wandb.ai/1249221036/huggingface/runs/xq5ajxm0


 17%|█▋        | 200/1164 [35:51<2:52:21, 10.73s/it]
 Loss per layer: [3.6797633171081543, 3.296281337738037, 2.974658727645874, 2.017885446548462], Total loss: 11.968588829040527
Loss per layer: [3.832794427871704, 3.3375542163848877, 2.8530211448669434, 1.9993431568145752], Total loss: 12.022712707519531
Loss per layer: [3.4228501319885254, 2.93503999710083, 2.5882346630096436, 1.7784008979797363], Total loss: 10.724525451660156
Loss per layer: [3.3639190196990967, 2.863468885421753, 2.522984743118286, 1.7248406410217285], Total loss: 10.475214004516602
Loss per layer: [3.9743597507476807, 3.2399775981903076, 2.7344305515289307, 1.8713167905807495], Total loss: 11.820084571838379
Loss per layer: [3.5517008304595947, 2.704529047012329, 2.3994598388671875, 1.6577726602554321], Total loss: 10.313462257385254
Loss per layer: [3.4316976070404053, 2.3825204372406006, 1.9924559593200684, 1.38333261013031], Total loss: 9.190006256103516
Loss per layer: [3.577561855316162, 2.2301554679870605, 1.9714899063110352, 1.372266173362732], Total loss: 9.151473045349121
Loss per layer: [3.7561724185943604, 2.171048402786255, 1.9657241106033325, 1.3535655736923218], Total loss: 9.24651050567627
Loss per layer: [3.6365413665771484, 2.1734468936920166, 2.0384278297424316, 1.4866265058517456], Total loss: 9.335041999816895
Loss per layer: [3.656924247741699, 2.212885618209839, 2.036478281021118, 1.471078872680664], Total loss: 9.37736701965332
Loss per layer: [3.692549228668213, 1.8550916910171509, 1.6633319854736328, 1.1543856859207153], Total loss: 8.365358352661133
Loss per layer: [3.246758460998535, 1.5704538822174072, 1.413112759590149, 0.9898990392684937], Total loss: 7.220223903656006
Loss per layer: [4.1397385597229, 1.899324893951416, 1.719080924987793, 1.2418174743652344], Total loss: 8.999961853027344
Loss per layer: [3.7184998989105225, 1.5005193948745728, 1.3502150774002075, 0.9494450688362122], Total loss: 7.518679618835449
Loss per layer: [3.407594680786133, 1.6392487287521362, 1.4911224842071533, 1.062928318977356], Total loss: 7.600893974304199
Loss per layer: [3.8605949878692627, 1.8965718746185303, 1.7920641899108887, 1.399070143699646], Total loss: 8.948301315307617
Loss per layer: [3.327787160873413, 1.512428879737854, 1.358404517173767, 1.0401463508605957], Total loss: 7.238767147064209
Loss per layer: [3.3643875122070312, 1.6874326467514038, 1.5492134094238281, 1.2486687898635864], Total loss: 7.84970235824585
Loss per layer: [3.508016347885132, 1.6041136980056763, 1.4905827045440674, 1.2600576877593994], Total loss: 7.862770080566406


 34%|███▍      | 400/1164 [1:18:12<2:59:12, 14.07s/it]
 {'eval_loss': 1.2090963125228882, 'eval_runtime': 96.5852, 'eval_samples_per_second': 20.707, 'eval_steps_per_second': 2.588, 'epoch': 0.51}
Loss per layer: [3.254413366317749, 1.3984241485595703, 1.2906218767166138, 1.0611311197280884], Total loss: 7.0045905113220215
Loss per layer: [3.8833489418029785, 1.7086492776870728, 1.574026346206665, 1.2818241119384766], Total loss: 8.447848320007324
Loss per layer: [3.6551356315612793, 1.6057723760604858, 1.4750761985778809, 1.2225040197372437], Total loss: 7.958488464355469
Loss per layer: [4.124615669250488, 1.6364879608154297, 1.518626093864441, 1.3178184032440186], Total loss: 8.597548484802246
Loss per layer: [3.6979422569274902, 1.7994863986968994, 1.6748491525650024, 1.4589120149612427], Total loss: 8.631190299987793
Loss per layer: [3.512904405593872, 1.5659383535385132, 1.4568145275115967, 1.2879478931427002], Total loss: 7.823604583740234
Loss per layer: [3.155095100402832, 1.2043756246566772, 1.0972228050231934, 0.9363566040992737], Total loss: 6.393050193786621
Loss per layer: [3.631654739379883, 1.4092042446136475, 1.3120172023773193, 1.201664686203003], Total loss: 7.55454158782959
Loss per layer: [3.7265114784240723, 1.284607172012329, 1.2057467699050903, 1.1514941453933716], Total loss: 7.368359565734863
Loss per layer: [3.5357518196105957, 1.5028347969055176, 1.3875551223754883, 1.1982136964797974], Total loss: 7.624355316162109
Loss per layer: [3.0663201808929443, 1.074385404586792, 0.9695886969566345, 0.883931040763855], Total loss: 5.99422550201416
Loss per layer: [3.2979013919830322, 1.4090770483016968, 1.2977557182312012, 1.1120048761367798], Total loss: 7.116738796234131
Loss per layer: [3.7013561725616455, 1.5118520259857178, 1.3774755001068115, 1.184351921081543], Total loss: 7.775035858154297
Loss per layer: [3.416447639465332, 1.3774077892303467, 1.260400652885437, 1.0830769538879395], Total loss: 7.137333393096924
Loss per layer: [3.782444715499878, 1.6832261085510254, 1.583971619606018, 1.358770728111267], Total loss: 8.40841293334961
Loss per layer: [3.4286341667175293, 1.360090970993042, 1.2452640533447266, 1.0873185396194458], Total loss: 7.121307373046875
Loss per layer: [4.0078253746032715, 1.6843132972717285, 1.5563279390335083, 1.3055088520050049], Total loss: 8.553975105285645
Loss per layer: [3.1266231536865234, 1.35420823097229, 1.2413722276687622, 1.0631200075149536], Total loss: 6.785323143005371
Loss per layer: [4.110530376434326, 1.4451496601104736, 1.3423948287963867, 1.1494107246398926], Total loss: 8.0474853515625
Loss per layer: [3.7398557662963867, 1.0305697917938232, 0.9040610194206238, 0.7961469888687134], Total loss: 6.470633506774902


 52%|█████▏    | 600/1164 [2:00:39<1:42:30, 10.91s/it]
 {'eval_loss': 1.1103230714797974, 'eval_runtime': 96.7495, 'eval_samples_per_second': 20.672, 'eval_steps_per_second': 2.584, 'epoch': 1.03}
Loss per layer: [3.9585554599761963, 1.3520547151565552, 1.2441699504852295, 1.1287084817886353], Total loss: 7.683488368988037
Loss per layer: [3.2549779415130615, 1.3264906406402588, 1.2068392038345337, 1.0239561796188354], Total loss: 6.8122639656066895
Loss per layer: [3.078083038330078, 1.3124014139175415, 1.1831709146499634, 1.059401273727417], Total loss: 6.633056640625
Loss per layer: [3.0371687412261963, 1.4514881372451782, 1.3417079448699951, 1.142473816871643], Total loss: 6.972838878631592
Loss per layer: [3.607421875, 1.321096420288086, 1.2028388977050781, 1.04014253616333], Total loss: 7.171499729156494
Loss per layer: [3.2301087379455566, 1.070198893547058, 0.9691481590270996, 0.8068399429321289], Total loss: 6.076295852661133
Loss per layer: [3.163973569869995, 1.2988824844360352, 1.1665998697280884, 1.0201587677001953], Total loss: 6.6496148109436035
Loss per layer: [3.323221206665039, 1.2622745037078857, 1.1496977806091309, 0.9841895699501038], Total loss: 6.719383239746094
Loss per layer: [3.4384422302246094, 1.3335694074630737, 1.2073010206222534, 1.0330358743667603], Total loss: 7.012348651885986
Loss per layer: [3.281642198562622, 1.2456437349319458, 1.1576403379440308, 1.0097805261611938], Total loss: 6.694706916809082
{'loss': 1.2594, 'learning_rate': 0.00018721804511278195, 'epoch': 1.29}
Loss per layer: [3.72520112991333, 1.2341490983963013, 1.1199300289154053, 0.9640185832977295], Total loss: 7.043298721313477
Loss per layer: [3.8939051628112793, 1.6073168516159058, 1.4966922998428345, 1.275258183479309], Total loss: 8.273172378540039
Loss per layer: [3.595200538635254, 1.6562548875808716, 1.546925663948059, 1.3292330503463745], Total loss: 8.12761402130127
Loss per layer: [3.137314796447754, 1.406842827796936, 1.2791610956192017, 1.105449914932251], Total loss: 6.928768157958984
Loss per layer: [3.8746695518493652, 1.6844327449798584, 1.5426428318023682, 1.3677995204925537], Total loss: 8.469544410705566
Loss per layer: [3.9430413246154785, 1.410127878189087, 1.2972685098648071, 1.154363989830017], Total loss: 7.804801940917969
Loss per layer: [4.193806171417236, 1.674004316329956, 1.552822470664978, 1.3982542753219604], Total loss: 8.818886756896973
Loss per layer: [3.4522078037261963, 1.426285982131958, 1.2938770055770874, 1.1574146747589111], Total loss: 7.329785346984863
Loss per layer: [3.2110002040863037, 1.256667971611023, 1.140880823135376, 1.0011333227157593], Total loss: 6.609682559967041
Loss per layer: [3.5364513397216797, 1.4369372129440308, 1.3251477479934692, 1.153509259223938], Total loss: 7.452045440673828


 69%|██████▊   | 800/1164 [2:45:23<1:08:43, 11.33s/it]
 {'eval_loss': 1.0962659120559692, 'eval_runtime': 96.8311, 'eval_samples_per_second': 20.655, 'eval_steps_per_second': 2.582, 'epoch': 1.54}
Loss per layer: [3.6291146278381348, 1.5272458791732788, 1.363905668258667, 1.1846590042114258], Total loss: 7.704925537109375
Loss per layer: [4.343428134918213, 1.3844096660614014, 1.2382911443710327, 1.1302975416183472], Total loss: 8.096426010131836
Loss per layer: [3.2345001697540283, 1.3630378246307373, 1.222607135772705, 1.0792040824890137], Total loss: 6.899349212646484
Loss per layer: [3.81123685836792, 1.390210509300232, 1.2774795293807983, 1.1703386306762695], Total loss: 7.649265766143799
Loss per layer: [3.8346025943756104, 1.5670093297958374, 1.4342079162597656, 1.2506574392318726], Total loss: 8.086477279663086
Loss per layer: [3.2678048610687256, 1.3355408906936646, 1.1847816705703735, 1.0382040739059448], Total loss: 6.826331615447998
Loss per layer: [3.7565386295318604, 1.5232551097869873, 1.3672970533370972, 1.2303204536437988], Total loss: 7.877411365509033
Loss per layer: [4.023116111755371, 1.4277763366699219, 1.30693781375885, 1.1716182231903076], Total loss: 7.929448127746582
Loss per layer: [3.5677335262298584, 1.3449580669403076, 1.2320157289505005, 1.0770477056503296], Total loss: 7.221755027770996
Loss per layer: [3.997180461883545, 1.362080454826355, 1.2312698364257812, 1.0641429424285889], Total loss: 7.6546735763549805
Loss per layer: [3.2851037979125977, 1.4579936265945435, 1.3286159038543701, 1.132029414176941], Total loss: 7.203742980957031
Loss per layer: [3.464231491088867, 1.4138509035110474, 1.307522177696228, 1.1555194854736328], Total loss: 7.341124057769775
Loss per layer: [3.356917142868042, 1.3361258506774902, 1.2010247707366943, 1.0038890838623047], Total loss: 6.897956848144531
Loss per layer: [3.1623764038085938, 1.3105762004852295, 1.192898154258728, 1.0241165161132812], Total loss: 6.689967632293701
Loss per layer: [3.463054895401001, 1.289168357849121, 1.1631391048431396, 1.029980182647705], Total loss: 6.945342540740967
Loss per layer: [3.450653314590454, 1.2067029476165771, 1.0990058183670044, 0.9751896858215332], Total loss: 6.731551647186279
Loss per layer: [4.586813926696777, 1.370090126991272, 1.2336311340332031, 1.0817055702209473], Total loss: 8.27224063873291
Loss per layer: [3.824592113494873, 1.268364429473877, 1.1578149795532227, 1.065276026725769], Total loss: 7.316047668457031
Loss per layer: [3.265974283218384, 1.1147809028625488, 1.0278288125991821, 0.8799878358840942], Total loss: 6.288571834564209
Loss per layer: [3.496190309524536, 1.45089852809906, 1.3084431886672974, 1.1654908657073975], Total loss: 7.421022415161133

                                                     

 86%|████████▌ | 1000/1164 [3:27:47<35:40, 13.05s/it]{'eval_loss': 1.0957924127578735, 'eval_runtime': 96.8105, 'eval_samples_per_second': 20.659, 'eval_steps_per_second': 2.582, 'epoch': 2.06}
Loss per layer: [3.4955801963806152, 1.382582426071167, 1.2828788757324219, 1.106100082397461], Total loss: 7.267141342163086
Loss per layer: [3.2445082664489746, 1.2074601650238037, 1.1218047142028809, 0.9941622614860535], Total loss: 6.567934989929199
Loss per layer: [3.6214025020599365, 1.4003105163574219, 1.2477538585662842, 1.1069066524505615], Total loss: 7.376374244689941
Loss per layer: [3.8827481269836426, 1.4788570404052734, 1.3466869592666626, 1.2294903993606567], Total loss: 7.937782287597656
Loss per layer: [3.1696202754974365, 1.1857378482818604, 1.0891973972320557, 0.9413996934890747], Total loss: 6.385954856872559
Loss per layer: [3.9429471492767334, 1.403261661529541, 1.2966645956039429, 1.1861540079116821], Total loss: 7.82902717590332
Loss per layer: [4.163066864013672, 1.4568549394607544, 1.3343770503997803, 1.1492773294448853], Total loss: 8.10357666015625
Loss per layer: [3.2131757736206055, 1.2330031394958496, 1.1247915029525757, 0.9790999889373779], Total loss: 6.550070762634277
Loss per layer: [3.451777219772339, 1.433292269706726, 1.3083101511001587, 1.1515692472457886], Total loss: 7.344948768615723
Loss per layer: [3.7288970947265625, 1.0023019313812256, 0.8660879731178284, 0.7969862818717957], Total loss: 6.394273281097412
Loss per layer: [3.6825547218322754, 1.3177400827407837, 1.2063347101211548, 1.0632853507995605], Total loss: 7.269914627075195
Loss per layer: [3.1286091804504395, 1.137819766998291, 1.0014023780822754, 0.8810981512069702], Total loss: 6.148929595947266
Loss per layer: [4.034530162811279, 1.2574201822280884, 1.0961483716964722, 1.0036709308624268], Total loss: 7.3917694091796875
Loss per layer: [3.316196918487549, 0.9196763038635254, 0.8126028776168823, 0.7170136570930481], Total loss: 5.7654900550842285
Loss per layer: [3.93194580078125, 1.2926549911499023, 1.1496261358261108, 1.0081764459609985], Total loss: 7.382403373718262
Loss per layer: [3.827711582183838, 1.3146755695343018, 1.1865888833999634, 1.0970871448516846], Total loss: 7.426063537597656
Loss per layer: [3.5327036380767822, 1.3714661598205566, 1.2669541835784912, 1.1155354976654053], Total loss: 7.286660194396973
Loss per layer: [3.7897849082946777, 1.3836255073547363, 1.246190071105957, 1.0963584184646606], Total loss: 7.515958786010742
Loss per layer: [3.9748220443725586, 1.4234238862991333, 1.3089778423309326, 1.1297391653060913], Total loss: 7.836963176727295
Loss per layer: [4.711394309997559, 1.886155366897583, 1.6893140077590942, 1.533859133720398], Total loss: 9.820722579956055
{'loss': 1.0862, 'learning_rate': 4.624060150375939e-05, 'epoch': 2.57}


100%|██████████| 1164/1164 [4:02:46<00:00, 12.51s/it]
{'eval_loss': 1.0955575704574585, 'eval_runtime': 96.8271, 'eval_samples_per_second': 20.655, 'eval_steps_per_second': 2.582, 'epoch': 2.57}
Loss per layer: [3.3706135749816895, 1.1855837106704712, 1.0564101934432983, 0.9498395919799805], Total loss: 6.5624470710754395
Loss per layer: [3.301589012145996, 1.5006706714630127, 1.3676483631134033, 1.2017184495925903], Total loss: 7.371625900268555
Loss per layer: [3.457439422607422, 1.4089741706848145, 1.27493417263031, 1.1078011989593506], Total loss: 7.249149322509766
Loss per layer: [4.543584823608398, 1.5576794147491455, 1.4278877973556519, 1.2757313251495361], Total loss: 8.804883003234863
Loss per layer: [3.2161474227905273, 1.3113462924957275, 1.2084070444107056, 1.040932536125183], Total loss: 6.7768330574035645
Loss per layer: [3.556967258453369, 1.3118257522583008, 1.2002273797988892, 1.0476187467575073], Total loss: 7.116639137268066
Loss per layer: [3.6156835556030273, 1.3483957052230835, 1.2210570573806763, 1.0539467334747314], Total loss: 7.239083290100098
Loss per layer: [3.405273199081421, 1.3682576417922974, 1.2205991744995117, 1.0509850978851318], Total loss: 7.0451154708862305
Loss per layer: [3.5763533115386963, 1.0442441701889038, 0.9287190437316895, 0.8589288592338562], Total loss: 6.408245086669922
Loss per layer: [3.7877559661865234, 1.5979315042495728, 1.4654550552368164, 1.3284327983856201], Total loss: 8.179574966430664
Loss per layer: [4.032683372497559, 1.4821513891220093, 1.329201340675354, 1.1736817359924316], Total loss: 8.017717361450195
Loss per layer: [3.695270538330078, 1.2070233821868896, 1.064353585243225, 0.9406728744506836], Total loss: 6.907320499420166
Loss per layer: [3.610565423965454, 1.2748658657073975, 1.1208763122558594, 1.0157170295715332], Total loss: 7.022024631500244
Loss per layer: [3.6655101776123047, 1.2944564819335938, 1.1709038019180298, 1.07318115234375], Total loss: 7.204051494598389
Loss per layer: [4.0920329093933105, 1.3907183408737183, 1.2211695909500122, 1.1319223642349243], Total loss: 7.835843086242676
Loss per layer: [3.2881176471710205, 1.2351869344711304, 1.1074368953704834, 0.9705187678337097], Total loss: 6.601259708404541
{'train_runtime': 14571.3798, 'train_samples_per_second': 10.245, 'train_steps_per_second': 0.08, 'train_loss': 1.1606226262358046, 'epoch': 2.99}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss █▂▁▁▁
wandb:                   eval/runtime ▁▆█▇█
wandb:        eval/samples_per_second █▃▁▂▁
wandb:          eval/steps_per_second █▃▁▁▁
wandb:                    train/epoch ▁▂▃▄▅▇▇█
wandb:              train/global_step ▁▂▃▄▅▇▇█
wandb:            train/learning_rate █▁
wandb:                     train/loss █▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 1.09556
wandb:                   eval/runtime 96.8271
wandb:        eval/samples_per_second 20.655
wandb:          eval/steps_per_second 2.582
wandb:                    train/epoch 2.99
wandb:              train/global_step 1164
wandb:            train/learning_rate 5e-05
wandb:                     train/loss 1.0862
wandb:               train/total_flos 1.539614312091353e+18
wandb:               train/train_loss 1.16062
wandb:            train/train_runtime 14571.3798
wandb: train/train_samples_per_second 10.245
wandb:   train/train_steps_per_second 0.08
wandb: 
wandb: 🚀 View run different-capybara-18 at: https://wandb.ai/1249221036/huggingface/runs/xq5ajxm0
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231006_124016-xq5ajxm0/logs
