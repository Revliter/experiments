# use pretrained heads, fix the heads and only train the lora part.

Training Alpaca-LoRA model with params:
gla: True
====================
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
layerwise: True
train_layers: [[28], [29], [30], [31]]
lr_scaling: [1.0, 1.0, 1.0, 1.0]
====================
base_model: decapoda-research/llama-7b-hf
data_path: yahma/alpaca-cleaned
output_dir: ../ckpts/pretrain_heads
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['(31|30|29|28).self_attn.q_proj', '(31|30|29|28).self_attn.v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca


Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:43<00:00,  1.33s/it]
Some weights of LlamaForCausalLM_GLA were not initialized from the model checkpoint at decapoda-research/llama-7b-hf and are newly initialized: ['gla_heads.1.weight', 'gla_heads.2.weight', 'gla_norms.0.weight', 'gla_heads.0.weight', 'gla_norms.2.weight', 'gla_norms.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 62.95it/s]
Loading cached split indices for dataset at /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c90bb7f249cf64b1.arrow and /home/users/nus/e0917621/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7b73a4086be540bf.arrow
trainable params: 524,288 || all params: 7,132,168,192 || trainable%: 0.0073510324754831585
Trainable parameters: ['base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight']

Map:   0%|          | 0/49760 [00:00<?, ? examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1991/2000 [00:02<00:00, 976.61 examples/s]
                                                                
wandb: Currently logged in as: 1249221036. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /scratch/users/nus/e0917621/zangwei/chenting/LLaMA-GLa/wandb/run-20231006_164711-mooc9oqt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-meadow-21
wandb: â­ï¸ View project at https://wandb.ai/1249221036/huggingface
wandb: ðŸš€ View run at https://wandb.ai/1249221036/huggingface/runs/mooc9oqt

  0%|          | 0/1164 [00:00<?, ?it/s]/home/users/nus/e0917621/.conda/envs/zangwei/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

 17%|â–ˆâ–‹        | 200/1164 [35:47<2:51:35, 10.68s/it]
 Loss per layer: [0.8244938850402832, 0.8060303330421448, 0.804709255695343, 1.9525901079177856], Total loss: 4.387823581695557
Loss per layer: [0.8255343437194824, 0.8135592341423035, 0.8234718441963196, 2.174989938735962], Total loss: 4.637555122375488
Loss per layer: [0.7917015552520752, 0.7678696513175964, 0.7646306753158569, 2.0293567180633545], Total loss: 4.353558540344238
Loss per layer: [0.9422697424888611, 0.9068981409072876, 0.8802788853645325, 1.5760300159454346], Total loss: 4.305476665496826
Loss per layer: [1.100050687789917, 1.0830800533294678, 1.0671944618225098, 1.6471084356307983], Total loss: 4.897433757781982
Loss per layer: [0.7971510291099548, 0.7902516722679138, 0.7734267711639404, 1.5576187372207642], Total loss: 3.918447971343994
Loss per layer: [1.0789557695388794, 1.0743016004562378, 1.038769245147705, 1.5672508478164673], Total loss: 4.75927734375
Loss per layer: [0.9836042523384094, 0.9101412892341614, 0.8703062534332275, 1.318535327911377], Total loss: 4.082587242126465
Loss per layer: [0.8595746159553528, 0.8222525119781494, 0.8178651928901672, 1.3273371458053589], Total loss: 3.827029228210449
Loss per layer: [0.8590962290763855, 0.8143459558486938, 0.8161817789077759, 1.2984120845794678], Total loss: 3.7880361080169678
Loss per layer: [0.7618424892425537, 0.7320765852928162, 0.7253159880638123, 0.9867312908172607], Total loss: 3.2059664726257324
Loss per layer: [0.9749539494514465, 0.9710750579833984, 0.92193204164505, 1.12787926197052], Total loss: 3.995840072631836
Loss per layer: [0.6681007146835327, 0.6261168122291565, 0.6335389614105225, 0.7484020590782166], Total loss: 2.6761586666107178
Loss per layer: [0.8689611554145813, 0.8432549834251404, 0.8137237429618835, 1.0017969608306885], Total loss: 3.5277369022369385
Loss per layer: [0.8288905024528503, 0.7687321305274963, 0.7635892629623413, 0.9386812448501587], Total loss: 3.2998929023742676
Loss per layer: [1.0378732681274414, 1.0015026330947876, 1.0103931427001953, 1.3412070274353027], Total loss: 4.3909759521484375
Loss per layer: [0.7596637010574341, 0.7461246848106384, 0.7212928533554077, 0.9556742906570435], Total loss: 3.182755470275879
Loss per layer: [0.9068120121955872, 0.9034427404403687, 0.8668723106384277, 1.2076609134674072], Total loss: 3.8847880363464355
Loss per layer: [0.9269214272499084, 0.9060596227645874, 0.8700960874557495, 1.113167643547058], Total loss: 3.8162450790405273
Loss per layer: [0.7293270230293274, 0.7001889944076538, 0.682799756526947, 0.8052796125411987], Total loss: 2.917595386505127

 34%|â–ˆâ–ˆâ–ˆâ–      | 400/1164 [1:15:00<2:33:42, 12.07s/it]
 {'eval_loss': 1.102060079574585, 'eval_runtime': 96.5576, 'eval_samples_per_second': 20.713, 'eval_steps_per_second': 2.589, 'epoch': 0.51}
Loss per layer: [0.8793514370918274, 0.9112887382507324, 0.8981127142906189, 1.0766369104385376], Total loss: 3.765389919281006
Loss per layer: [1.1793955564498901, 1.1723768711090088, 1.158210277557373, 1.3039828538894653], Total loss: 4.813965320587158
Loss per layer: [0.9599508047103882, 0.9778160452842712, 0.9531351923942566, 1.108149528503418], Total loss: 3.999051570892334
Loss per layer: [1.1532503366470337, 1.1741074323654175, 1.1596879959106445, 1.3070603609085083], Total loss: 4.7941060066223145
Loss per layer: [0.8146506547927856, 0.7892778515815735, 0.7868117690086365, 0.9817302823066711], Total loss: 3.3724706172943115
Loss per layer: [0.6058539152145386, 0.5694283246994019, 0.563017725944519, 0.9191116690635681], Total loss: 2.657411575317383
Loss per layer: [0.9049770832061768, 0.8698422312736511, 0.8696362376213074, 1.1235551834106445], Total loss: 3.7680108547210693
Loss per layer: [0.9161076545715332, 0.885648250579834, 0.8713663220405579, 1.0296883583068848], Total loss: 3.702810525894165
Loss per layer: [0.9018514752388, 0.8885465860366821, 0.8702228665351868, 1.1226632595062256], Total loss: 3.7832841873168945
Loss per layer: [1.0637199878692627, 1.0739071369171143, 1.054194450378418, 1.2102336883544922], Total loss: 4.402055263519287
Loss per layer: [0.9396419525146484, 0.9077223539352417, 0.8975394368171692, 1.0920072793960571], Total loss: 3.836911201477051
Loss per layer: [1.0559152364730835, 1.006591558456421, 1.0062001943588257, 1.2084959745407104], Total loss: 4.27720308303833
Loss per layer: [0.8616663217544556, 0.8457172513008118, 0.7992247939109802, 1.0271100997924805], Total loss: 3.5337185859680176
Loss per layer: [0.9822385907173157, 0.941017746925354, 0.9485467672348022, 1.273494005203247], Total loss: 4.145297050476074
Loss per layer: [0.9987762570381165, 0.989332377910614, 0.9553399682044983, 1.1825282573699951], Total loss: 4.1259765625
Loss per layer: [0.7758749723434448, 0.7527821063995361, 0.7467537522315979, 0.9559325575828552], Total loss: 3.2313435077667236
Loss per layer: [0.9542763233184814, 0.9383429288864136, 0.898337721824646, 1.070282220840454], Total loss: 3.861239194869995
Loss per layer: [1.0488587617874146, 1.0347611904144287, 1.032792568206787, 1.2186682224273682], Total loss: 4.335081100463867
Loss per layer: [0.9327107071876526, 0.9055430889129639, 0.8882739543914795, 0.9589779376983643], Total loss: 3.6855056285858154
Loss per layer: [0.8236234188079834, 0.8084645867347717, 0.823883056640625, 0.9730587005615234], Total loss: 3.429029703140259


 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 600/1164 [1:56:16<1:42:25, 10.90s/it]
 {'eval_loss': 1.0696660280227661, 'eval_runtime': 96.6551, 'eval_samples_per_second': 20.692, 'eval_steps_per_second': 2.587, 'epoch': 1.03}
Loss per layer: [1.011379361152649, 0.9402318596839905, 0.9414677619934082, 1.0701037645339966], Total loss: 3.9631829261779785
Loss per layer: [0.9128948450088501, 0.8933200836181641, 0.9056569933891296, 1.0872424840927124], Total loss: 3.799114227294922
Loss per layer: [0.953493595123291, 0.9390203952789307, 0.9164714813232422, 1.123633861541748], Total loss: 3.932619333267212
Loss per layer: [0.9138973951339722, 0.8570748567581177, 0.8438959717750549, 0.9557315111160278], Total loss: 3.5705995559692383
Loss per layer: [1.0530544519424438, 1.0088399648666382, 1.0216271877288818, 1.2431604862213135], Total loss: 4.326682090759277
Loss per layer: [0.7101714611053467, 0.7322793006896973, 0.7142928838729858, 0.9990976452827454], Total loss: 3.155841112136841
Loss per layer: [0.7930586338043213, 0.7821410298347473, 0.7902308702468872, 1.125691533088684], Total loss: 3.491121768951416
Loss per layer: [0.6621613502502441, 0.6186826229095459, 0.617093026638031, 0.8141857385635376], Total loss: 2.712122917175293
Loss per layer: [1.0683841705322266, 1.0510718822479248, 1.0270053148269653, 1.1930867433547974], Total loss: 4.339548110961914
Loss per layer: [0.9204756617546082, 0.9061012864112854, 0.8851419687271118, 1.0236114263534546], Total loss: 3.735330581665039
{'loss': 1.2027, 'learning_rate': 0.00018721804511278195, 'epoch': 1.29}
Loss per layer: [0.9449271559715271, 0.915511429309845, 0.9236894249916077, 1.102105975151062], Total loss: 3.8862338066101074
Loss per layer: [0.8234203457832336, 0.8180789947509766, 0.7749603986740112, 0.9381694197654724], Total loss: 3.3546290397644043
Loss per layer: [0.9224628806114197, 0.891045868396759, 0.8696878552436829, 1.1091548204421997], Total loss: 3.792351245880127
Loss per layer: [1.146041750907898, 1.0813006162643433, 1.0449025630950928, 1.13680899143219], Total loss: 4.409053802490234
Loss per layer: [1.018839955329895, 1.0118961334228516, 0.9848052263259888, 1.1625293493270874], Total loss: 4.178070545196533
Loss per layer: [1.0182163715362549, 1.0187573432922363, 1.0259742736816406, 1.2424250841140747], Total loss: 4.305373191833496
Loss per layer: [0.8768194317817688, 0.8492326140403748, 0.8446540236473083, 0.9832829833030701], Total loss: 3.5539891719818115
Loss per layer: [0.9713950157165527, 0.9570466876029968, 0.9579393863677979, 1.1671003103256226], Total loss: 4.053481578826904
Loss per layer: [0.8626617193222046, 0.8328496813774109, 0.8063032031059265, 1.0282539129257202], Total loss: 3.5300683975219727
Loss per layer: [0.8068883419036865, 0.7682563662528992, 0.7758409380912781, 0.9332197904586792], Total loss: 3.284205436706543

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 800/1164 [2:38:20<1:10:08, 11.56s/it]
 {'eval_loss': 1.0578562021255493, 'eval_runtime': 96.5807, 'eval_samples_per_second': 20.708, 'eval_steps_per_second': 2.589, 'epoch': 1.54}
Loss per layer: [0.7885257601737976, 0.8011622428894043, 0.7894914150238037, 1.059775710105896], Total loss: 3.438955307006836
Loss per layer: [0.9126856923103333, 0.8924764394760132, 0.8761322498321533, 1.1119362115859985], Total loss: 3.7932305335998535
Loss per layer: [1.004874348640442, 0.9533216953277588, 0.9689463973045349, 1.13774573802948], Total loss: 4.064888000488281
Loss per layer: [0.9307759404182434, 0.8990436792373657, 0.8850482106208801, 1.043467402458191], Total loss: 3.7583351135253906
Loss per layer: [0.9825496673583984, 0.9527656435966492, 0.9291397929191589, 1.115281105041504], Total loss: 3.979736328125
Loss per layer: [0.9358850121498108, 0.9208908081054688, 0.9114602208137512, 1.0613815784454346], Total loss: 3.829617500305176
Loss per layer: [0.8499346971511841, 0.8177596926689148, 0.8117327690124512, 0.9393702745437622], Total loss: 3.418797492980957
Loss per layer: [1.0193195343017578, 0.9785875082015991, 0.9457322955131531, 1.018344759941101], Total loss: 3.961984157562256
Loss per layer: [0.86078941822052, 0.8675469160079956, 0.8654190897941589, 0.9937537312507629], Total loss: 3.5875091552734375
Loss per layer: [0.7111156582832336, 0.7075939178466797, 0.7089311480522156, 0.919037938117981], Total loss: 3.0466785430908203
Loss per layer: [0.9479963183403015, 0.9021514654159546, 0.8942478895187378, 1.0214836597442627], Total loss: 3.7658793926239014
Loss per layer: [0.9988372921943665, 0.9642221927642822, 0.9139882922172546, 0.9742727279663086], Total loss: 3.851320505142212
Loss per layer: [0.9092473387718201, 0.8784008622169495, 0.8397470116615295, 1.0949618816375732], Total loss: 3.7223570346832275
Loss per layer: [1.0535136461257935, 1.047086238861084, 1.0216513872146606, 1.2014068365097046], Total loss: 4.323657989501953
Loss per layer: [0.8886780738830566, 0.8848808407783508, 0.833975076675415, 0.9961633086204529], Total loss: 3.6036972999572754
Loss per layer: [0.6301254630088806, 0.6120525002479553, 0.6186536550521851, 0.915727436542511], Total loss: 2.7765591144561768
Loss per layer: [0.9351719617843628, 0.8924056887626648, 0.8841217160224915, 1.0682048797607422], Total loss: 3.7799041271209717
Loss per layer: [0.9165120720863342, 0.9011741280555725, 0.894635796546936, 1.0616374015808105], Total loss: 3.7739593982696533
Loss per layer: [0.9388817548751831, 0.9244370460510254, 0.8953322172164917, 1.0348645448684692], Total loss: 3.793515682220459
Loss per layer: [1.0085110664367676, 0.9861428737640381, 0.9719092845916748, 1.1762211322784424], Total loss: 4.142784118652344
                                                     

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1000/1164 [3:18:53<36:41, 13.42s/it]
 {'eval_loss': 1.0521173477172852, 'eval_runtime': 96.8654, 'eval_samples_per_second': 20.647, 'eval_steps_per_second': 2.581, 'epoch': 2.06}
Loss per layer: [1.0468454360961914, 0.984489917755127, 0.9826037883758545, 1.1383405923843384], Total loss: 4.152279853820801
Loss per layer: [0.5824853181838989, 0.5665768980979919, 0.5708827376365662, 0.7759097218513489], Total loss: 2.495854616165161
Loss per layer: [0.9795630574226379, 0.9612507820129395, 0.930669903755188, 1.0848947763442993], Total loss: 3.95637845993042
Loss per layer: [0.89165198802948, 0.8893439173698425, 0.8774484395980835, 1.0965654850006104], Total loss: 3.755009889602661
Loss per layer: [0.9815847277641296, 0.940629243850708, 0.9334986209869385, 0.983658492565155], Total loss: 3.8393712043762207
Loss per layer: [0.8890834450721741, 0.8774017691612244, 0.878818929195404, 0.9870600700378418], Total loss: 3.632364273071289
Loss per layer: [0.9655284881591797, 0.9360463619232178, 0.9115357995033264, 1.1360565423965454], Total loss: 3.949167251586914
Loss per layer: [0.8067942261695862, 0.7889637351036072, 0.7419446706771851, 0.9626309275627136], Total loss: 3.3003337383270264
Loss per layer: [0.833797812461853, 0.8352328538894653, 0.8061364889144897, 0.9911141395568848], Total loss: 3.4662814140319824
Loss per layer: [0.4849388599395752, 0.4605933725833893, 0.45304957032203674, 0.7209814190864563], Total loss: 2.119563102722168
Loss per layer: [0.8939070105552673, 0.8429365158081055, 0.8028167486190796, 1.0207042694091797], Total loss: 3.5603647232055664
Loss per layer: [0.9842677116394043, 0.9371777176856995, 0.916191577911377, 1.0608620643615723], Total loss: 3.898499011993408
Loss per layer: [0.9688794612884521, 0.9698150157928467, 0.9357956051826477, 1.0231890678405762], Total loss: 3.897679090499878
Loss per layer: [0.8749479651451111, 0.8616227507591248, 0.8483818173408508, 1.0462523698806763], Total loss: 3.6312050819396973
Loss per layer: [0.8768781423568726, 0.825608491897583, 0.8307082056999207, 0.9594470262527466], Total loss: 3.4926419258117676
Loss per layer: [0.9422913789749146, 0.9136489629745483, 0.8904017210006714, 1.081692099571228], Total loss: 3.8280344009399414
Loss per layer: [0.9265065789222717, 0.9245373010635376, 0.8996275067329407, 1.0092130899429321], Total loss: 3.7598843574523926
Loss per layer: [0.897634744644165, 0.8915494084358215, 0.8713527321815491, 1.0262526273727417], Total loss: 3.6867895126342773
Loss per layer: [0.8015729188919067, 0.7718670964241028, 0.7554683685302734, 0.9503570795059204], Total loss: 3.2792654037475586
Loss per layer: [0.8352537155151367, 0.8230476975440979, 0.7578629851341248, 1.02395498752594], Total loss: 3.4401192665100098
{'loss': 1.0447, 'learning_rate': 4.624060150375939e-05, 'epoch': 2.57}

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1164/1164 [3:55:19<00:00, 12.13s/it]
{'eval_loss': 1.0495383739471436, 'eval_runtime': 96.6732, 'eval_samples_per_second': 20.688, 'eval_steps_per_second': 2.586, 'epoch': 2.57}
Loss per layer: [0.8112324476242065, 0.7638980746269226, 0.7550916075706482, 0.8907758593559265], Total loss: 3.2209980487823486
Loss per layer: [1.0668948888778687, 1.0255351066589355, 1.0152573585510254, 1.1479703187942505], Total loss: 4.25565767288208
Loss per layer: [0.7828267216682434, 0.7390938401222229, 0.7568122148513794, 0.9662081599235535], Total loss: 3.244940996170044
Loss per layer: [0.8340118527412415, 0.7993683218955994, 0.7657292485237122, 0.9298276901245117], Total loss: 3.32893705368042
Loss per layer: [0.9828289747238159, 0.9700780510902405, 0.9381904006004333, 1.1613128185272217], Total loss: 4.052410125732422
Loss per layer: [0.7802274823188782, 0.7688851356506348, 0.7543200254440308, 0.9854555726051331], Total loss: 3.2888879776000977
Loss per layer: [0.7334503531455994, 0.7407068610191345, 0.7035651803016663, 0.9443433284759521], Total loss: 3.122065782546997
Loss per layer: [0.9468423128128052, 0.9307100176811218, 0.9319922924041748, 1.2060638666152954], Total loss: 4.015608310699463
Loss per layer: [0.6711857318878174, 0.633865475654602, 0.6207571029663086, 0.7804833054542542], Total loss: 2.706291675567627
Loss per layer: [0.7205812335014343, 0.6938362121582031, 0.6972735524177551, 0.8574023246765137], Total loss: 2.9690933227539062
Loss per layer: [0.8251984715461731, 0.821354866027832, 0.8040816187858582, 0.9391693472862244], Total loss: 3.3898043632507324
Loss per layer: [0.9670698642730713, 0.9522514939308167, 0.9468415379524231, 1.1208345890045166], Total loss: 3.986997365951538
Loss per layer: [1.015596866607666, 1.0040619373321533, 0.9864067435264587, 1.1711019277572632], Total loss: 4.1771674156188965
Loss per layer: [0.6934736371040344, 0.6869925260543823, 0.6613844037055969, 0.9011389017105103], Total loss: 2.9429893493652344
Loss per layer: [0.7521377801895142, 0.7351376414299011, 0.7024813890457153, 0.9304872155189514], Total loss: 3.120244026184082
Loss per layer: [0.9727125763893127, 0.9561575651168823, 0.9091070294380188, 1.0742621421813965], Total loss: 3.9122393131256104
{'train_runtime': 14125.031, 'train_samples_per_second': 10.568, 'train_steps_per_second': 0.082, 'train_loss': 1.1114998125948037, 'epoch': 2.99}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                      eval/loss â–ˆâ–„â–‚â–â–
wandb:                   eval/runtime â–â–ƒâ–‚â–ˆâ–„
wandb:        eval/samples_per_second â–ˆâ–†â–‡â–â–…
wandb:          eval/steps_per_second â–ˆâ–†â–ˆâ–â–…
wandb:                    train/epoch â–â–‚â–ƒâ–„â–…â–‡â–‡â–ˆ
wandb:              train/global_step â–â–‚â–ƒâ–„â–…â–‡â–‡â–ˆ
wandb:            train/learning_rate â–ˆâ–
wandb:                     train/loss â–ˆâ–
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                      eval/loss 1.04954
wandb:                   eval/runtime 96.6732
wandb:        eval/samples_per_second 20.688
wandb:          eval/steps_per_second 2.586
wandb:                    train/epoch 2.99
wandb:              train/global_step 1164
wandb:            train/learning_rate 5e-05
wandb:                     train/loss 1.0447
wandb:               train/total_flos 1.5407595794108252e+18
wandb:               train/train_loss 1.1115
wandb:            train/train_runtime 14125.031
wandb: train/train_samples_per_second 10.568
wandb:   train/train_steps_per_second 0.082
wandb: 
wandb: ðŸš€ View run true-meadow-21 at: https://wandb.ai/1249221036/huggingface/runs/mooc9oqt
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231006_164711-mooc9oqt/logs
